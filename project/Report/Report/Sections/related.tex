% Exahype templating
% make mroe systems, less user friendly


% Euler 3D SIMD speedup 1.04
% CCZ4 1.27

Much of ExaHyPE's code generation is preformed via templating.
Templating is a powerful, yet simple method to generate large sections of glue code.
Although as discussed in section \ref{sec:problem_statement}, templating along the boundary of user and engine code leaves opportunities for improvements.  
A solution to this problems was proposed by \citeauthor{templateExahype} by extending the domain of templating to user code \cite{templateExahype}.

\citeauthor{templateExahype} observed that while creating an ExaHyPE projects there are often 3 separable roles preformed by users.
These were: application experts, who focus on configuration of ExaHyPE; algorithm experts, who understand and implement the PDEs; and optimisation experts, who work at a lower level to increase performance.
The idea behind further templating is to allow these roles, which may be filled by different people, to operate synergically.
Increased templating also offers a convenient way to implement architecture aware optimisations, increasing code portability and speed.
\citeauthor{templateExahype} showed an example use case of their technique that applied a SIMD friendly SoA to AoS transformation to user code.
This transformation was tested on 2 problems: the Euler equations in 3D, and the Einstein equations from relativistic astrophysics (CCZ4).
They found that the memory bound Euler 3D problem experienced a $1.05\times$ speedup and the compute bound CCZ4 equations experienced a $1.27\times$ speedup.

While templating serves a vital role within ExaHyPE, we do not believe it to be the best way to optimise user code.
Templating inherently requires an optimisation expert for any performance improvements to be realized.
Our compiler based approach is based off the idea that conventional compilers are exceptionally powerful, and they should take the role of optimisation expert, not a person. 



% Yateto used a compiler
YATeTo (Yet Another Tensor Toolbox) \cite{YATeTo}, shares a similar philosophy to this paper, however YATeTo's approach relies on GEMM (General Matrix-Matrix Multiplication) libraries wheras we rely on conventional compilers alone.
YATeTo operates in the domain of linear hyperbolic PDEs, in partial, it is part of the Seisol project \cite{seisolPFLOP}, which focuses on applying the linear elastic wave HPDE to model seismic activity.
However, YATeTo is a general Tensor Toolbox and can be used for far more general applications.

The Discontinuous Galerkin method often used to to solve HPDEs can, in the linear case, be discretized into a series of Tensor contractions.
YATeTo offers a Domain Specific Language (DSL) within python for users to describe their tensor contractions.
Then through a series of compilation steps outputs C++, which crucially contains many calls to GEMM libraries.
Highly performant GEMM libraries and BLAS (Basic Linear Algebra Subprograms) libraries are often distributed by system vendors.
These libraries are optimised for a wie range of input problems sizes, and often include hardware specific optimisations like improved cache usage.
It is unlikely that a user of these libraries could write code that was more performant than the library, and even if they could they would have to write new code for every system they used.
Hence, utlizing GEMM libraries in abundence can drastically speedup code.

Furthermore, YATeTo preforms optimisations within its compilation steps, such as strength reduction, sparsity patten exploitation and memory layout optimisation.
Overall, YATeTo achieved a speedups between $1.1\times$ to $6.5 \times$ within SeiSol, which is impressive.
YATeTo shows that automation of the optimisation process using a compiler can be very effective.
Especially while using high performance, system agnostic libraries such as GEMM.
Unfortunately, the use of tensor contractions cannot be carried over to non-linear HPDEs, due to their non-linearity.
Hence, we explore the more general technique of leveraging conventional compilers.   



% firedrake used a compiler
% Hummm
%\cite{FiredrakeAndCOFFEE}


%Auto code gen
% SIMD - problem specific
However, YATeTo is not alone in the space of automatic optimisations using a compiler based approach.
\citeauthor{codegen_dg_SIMD} explore a similar linear domain as YATeTo in their paper that addresses optimising for modern architectures \cite{codegen_dg_SIMD}. 
Although they choose to preform SIMD vectorisation within their compiler, introducing SIMD intrinsics to the output code, as opposed to YATeTo that uses GEMM libraries.
\citeauthor{codegen_dg_SIMD} state that there are 2 paths to introduce vectorisation within code.
Automatic vectorisation through conventional compilers, and explicit vectorisation where SIMD instrisic instructions are introduced by developers.
They argue that although the former is preferable, the latter is will be more performant due to a developers problem specific knowledge.
As such, they introduce a compiler architecture that automatically preform explicit vectorisation, using observations and deductions that arise from the tensor decomposition of linear HPDEs.

Again, our requirement of supporting non-linear HPDEs prohibits us from exploiting tensor contractions, and instead exposes us to a more varied range of inputs.
As such, we choose to use the former of  \citeauthor{codegen_dg_SIMD} path to vectorisation, relying entirely on the conventional compiler.
However, this paper does show that users of \phlat may wish to explore the use of explicit techniques and problem specific knowledge to improve the performance of their programs.
Hence, as a design requirement \phlat should support this.
\citeauthor{codegen_dg_SIMD} found that their explicit vectorisation achived 50\% peak floating point performance solving the diffusion-reaction Equation on an Intel Haswell system.
Although, they noted that there was potential for further performance improvements with addition tweaking.  
