% PDEs - what they are
Many physical phenomena can be described by partial differential equations (PDEs) including:  seismic wave propagation \cite{earthquakePDE}, fluid dynamics \cite{exahype}, or relativistic astrophysics \cite{relativisticPDE}.
Consequently, solutions to PDEs can have a profounds affects, for example, by improving our understanding of tsunami \cite{tsunamiPDE}.


Analytical solutions to PDEs are difficult to produce, and general analytical solutions for PDEs that cover varying initial conditions and boundary conditions do not exist.
As such, we rely on numerical methods to calculate approximate solutions to PDEs.
However, numerical methods are computationally intensive - increasing the size of the problem, or the accuracy of the solution, will increase the computational work.
Many real world problems require billions of degrees of freedom.
The sheer scale of these computations requires the use of high performance computing (HPC), where algorithmic advances and supercomputers make these problems tractable.

% PDEs - some math, in particular flux
Our will be on using the Finite Volume scheme to solve hyperbolic PDEs (HPDEs).
In FV the spacial domain is decomposed into small volumes called cells.
Each of these cells has a state, this state mirrors the unknowns of the PDE.
The state of these cells are effected by source terms, boundary conditions and flux between neighboring cells.




% Exhaype - what it is
Implementing a highly scalable FV solver is a non-trivial task, taking research teams months, or even years to create \cite{tensorChemistry}.
PDE engines offer a solution to this problem, by providing a generalised framework to build solvers for many different problems.
We will be using ExaHyPE - An Exascale Hyperbolic PDE Engine \cite{exahype}.
ExaHyPE's domain is linear and non-linear hyperbolic systems of PDEs (HPDEs) written in first order form. 
ExaHyPE can used both the FV and ADER-DG (Arbitrary high-order using Derivatives - Discontinuous Galerkin) numerical schemes.
And ExaHyPE uses the Peano framework \cite{PeanoFramework} to divide and distribute the spacial domain into smaller patches to be run across multiple process and compute nodes.

To use ExaHyPE, a user begins by describing their problem to the ExaHyPE toolkit using a python interface.
A user will specify things such as: how many unknowns and auxillary variables are used; weather their problem uses a non-conservative product (NCP); what numerical scheme they want to use; if they want to use fix or adaptive time stepping; how frequently should the solution be plotted; and more.
This is used by the ExaHyPE toolkit to generate a project.
The project is ready to be compiled, however is missing the numerical problem description.
The project contains placeholder functions that calculate flux, eigenvalues, initial conditions, boundary conditions and non-conservative products (if applicable).   
These must be filled in by the user.
Once these placeholder functions have been filled in, the project can be compiled and will solved the users problem.


% Exahype - benifits of exahype (fast flexible, friendly)
At ExaHyPEs core it is fast, flexible and user-friendly, allowing users to create fast programs to solve a wide range of problems while requiring minimal effort for a user.
However, as the quality of tooling increases so to does the ambition of users, who seek to tackle larger and more complex problems.
Hence, there will always be the desire to further improve the tooling to meet these loftier ambitions.
This paper will focus on developing a general technique that can be used to accelerate the speed of users code, and reduce the onus for a user to invest time in optimisation.
We will use a compiler based approach, which in turn leverages the power of conventional compilers such as gcc.
We will use the terms ``out compiler'' and ``conventional compilers'' to disambiguate the compiler this paper proposes to conventional compilers like \texttt{gcc} \texttt{clang} \texttt{ipcx}.  


% Shout out to compilers
Conventional compliers are very complex, and tremendously powerful.
\texttt{gcc} has over 100 optimisation flags, encompassing a huge variety of optimisations, from loop unrolling to automatic SIMD vectorisation to switching floating point operation modes.
There are continuing advances in the field of compiler optimisation spanning topics such as cache allocation and JIT optimisation \cite{weak-compiler-survey}.
Furthermore, there are recent attempts to use machine learning to improve compilers, where ultimately compiling is just an difficult optimisation problem with a set of constraints (e.g. compiled output must represent the input) \cite{compiler-ml-opt}.
As the cutting edge of compiler research moves ever forward,we can expect these advancements to make their way into production compilers such as gcc.
Therefore, a valid strategy for further optimising code can be to simply present your code in a way preferred by your compiler.

This is the core idea explored in this paper.
We propose generating compute kernels as huge monolithic functions, as opposed to hand written code littered with human friendly abstraction.
And we explore how this change in problem representation effects the performance of code generated by conventional compilers.
