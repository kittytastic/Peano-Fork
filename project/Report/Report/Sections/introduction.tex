% PDEs - what they are
Many physical phenomena can be described by partial differential equations (PDEs) including:  seismic wave propagation \cite{earthquakePDE}, fluid dynamics \cite{exahype}, or relativistic astrophysics \cite{relativisticPDE}.
As such, solutions to PDEs can have much real world impact by, for example, improving tsunami modeling \cite{tsunamiPDE}.


Analytical PDE solutions are challenging to derive, and general solutions for any initial conditions or boundary conditions do not exist.
As such, we rely on numerical methods to calculate approximate solutions.
However, numerical methods are computationally intensive; increasing the size of the problem, or the accuracy of the solution, increases the computational work.
Many real world problems require billions of degrees of freedom to model.
The sheer scale of these computations requires the use of high performance computing (HPC), where algorithmic advances and supercomputers make these problems tractable.

In HPC, there are 2 ways to increase computational throughput, either by using more hardware, or by using available hardware more efficiently.
Super computing hardware is ever improving, and Exa-Scale systems are on the horizon.
However, relying on hardware scaling comes with the cost and availability issues around using more hardware.
Therefore, optimising code to better use the hardware is often preferable.
In HPC codes there are many levels at which optimisation can occur, one of the most fundamental is improving the throughput of a single core.
There are many example in literature of this having a significant effect on the performance of HPC code \cite{YATeTo,seisolPFLOP,codegen_dg_SIMD}, and this will be the area of optimisation we explore.     

% Exhaype - what it is
Our focus will be on the per core performance of Finite Volume (FV) kernels used to solve PDEs within ExaHyPE - An Exascale Hyperbolic PDE Engine \cite{exahype}.
Implementing a highly scalable FV solver is a non-trivial task, and can take research teams months, or even years to create \cite{tensorChemistry}.
This is the problem that ExaHyPE solves by providing a generalised framework to create solvers for many different problems.
ExaHyPE's domain is linear and non-linear hyperbolic systems of PDEs written in first order form.

ExaHyPE offers many features to its users.
ExaHyPE supports both the FV and ADER-DG (Arbitrary high-order using Derivatives - Discontinuous Galerkin) numerical schemes.
Along with supporting, both adaptive or fixed time stepping and adaptive or fixed meshes.
The Peano framework \cite{PeanoFramework} lies at the core of ExaHyPE.
This is responsible for dividing the spacial domain into smaller patches and distributing these patches across computing resources.
Every patch is updated by a single core using the FV scheme.
It is this update process we will optimise.

To use ExaHyPE, a user begins by describing their problem to the ExaHyPE toolkit using a python interface.
A user will specify information such as: how many unknowns and auxillary variables are used; weather their problem uses a non-conservative product (NCP); what numerical scheme they want to use; if they want to use fix or adaptive time stepping; how frequently the solution should be plotted; and more.
This is used by the ExaHyPE toolkit to generate a project.
This project is primarily populated by glue code that calls ExaHyPE, however there are a few placeholders function that need to be filed in by the user.
These place holder functions are used to describe the PDE, calculating: flux, eigenvalues, initial conditions, boundary conditions, and non-conservative products (if applicable).
Upon completing these placeholder functions, the project can be compiled and run on systems ranging from a laptop to a supercomputer.

% Exahype - benifits of exahype (fast flexible, friendly)
At ExaHyPEs core it is fast, flexible and user-friendly, allowing users to create fast programs to solve a wide range of problems while requiring minimal effort from the user.
However, as the quality of tooling increases so to does the ambition of users, who seek to tackle larger and more complex problems.
Hence, there will always be the desire to further improve the tooling to meet these loftier ambitions.
This paper will focus on developing a general technique that can be used to accelerate the speed of users code, and reduce the onus for a user to invest time in optimisation.
We will use a compiler based approach, which in turn leverages the power of conventional compilers such as gcc.
The compiler we present is called \phlat, its name reflecting its approach to code generation, producing code that is: Flat Long And potentially Transformed. 
We use the term \textit{conventional compiler} to refer to optimismising compilers such as \texttt{gcc}, \texttt{clang}, \texttt{ipcx}.  


% Shout out to compilers
The effects of conventional compliers on HPC codes are often overlooked, and they are rarely afforded the recognition they deserve.
It is a given that HPC codes will be compiled with \texttt{-O3} or \texttt{-Ofast} flags.
However, if we take a step back to appreciate this, by simply adding a single flag to our compiler arguments we have achieved an order (or a few orders) of magnitude speedup.
This is effectively free performance.
And the benefits don't stop there, vendor specific compilers like Intel's \texttt{ipcx} and AMD's AOCC, offer hardware specific optimisations.
So by switching compiler you can create a highly efficient binary that targets specific hardware, no coding required.

It is possible to beat the conventional compiler with manual optimisation, especially at higher levels of abstraction.
However, it gets increasingly more challenging (and impractical to try) as you approach assembly.
And conventional compilers are only getting more advanced, narrowing this gap.   
Much current research focuses on using Machine learning to improve compilers \cite{compiler-ml-opt,lots-of-compiler-options}.
And developments such as CompilerGym, an environment for exploring compiler optimisation in AI research \cite{compiler-gym}, are indicators that this field of research will continue to expand. 

As such, we propose that a valid method of HPC codes optimisation is to simply use the conventional compiler better.
Our \phlat compiler has a simple goal, to transform user codes into compiler friendly codes.    
In partial we focus on generating monolithic functions, from FV kernels.

% what is in each section
In the next section we will introduce the problem within ExaHyPE that \phlat is used to solve, followed by discussing similar compiler based approaches.
In Section \ref{sec:methodology} we explain the architecture of \phlat and how it is used to generate compute kernels.
And in Section \ref{sec:results} we will discuss the performance of kernels generated along with the practicalities of using \phlat. 