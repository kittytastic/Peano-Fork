\subsection{Testing Methodology}
% Test methodology
% [ ] Problems
% [ ] Kernel Validity
% [ ] Exahype results
% [ ] Kernel Compare results

We will use the Euler equations in 2D and 3D, along with the shallow water equations to test our compiler. 
A summary of these problems can be found below.

\vspace{1em}
\begin{tabular}{ccccccccc}
Problem & Unknowns+Auxillary & Dim & Patch size & Uses Flux & Uses NCP\\
\hline
Euler 2D & 4+0 & 2 & 3x3 & \checkmark & \xmark \\
Euler 3D & 5+0 & 3 & 3x3x3 & \checkmark & \xmark \\
SWE & 3+1 & 2 & 3x3 & \checkmark & \checkmark \\
\end{tabular}
\vspace{1em}

These problems were chosen as they are prototypical hyperbolic PDE test problems that exhibit a wide range of features, such as the use of 2D and 3D along with the use of NCP and auxiliary variables.
Also, each of theses problems are given as example within the ExaHyPE repository.
The kernels from these examples solutions will be treated as a control for the compiled kernels.
We will refer to these kernels as default kernels. 

None of the test problems are compute bound problems, they are all memory bound problems.
This means that within the an ExaHyPE program we would not expect increasing the speed of the kernel to have a large effect on the total runtime of the program, as the program is bottle necked by memory throughput.

As will be discussed in Section \ref{sec:practical} creating input DAGs in a non-trivial job, and the size of DAGs that compute bound problems such as the CCZ4 equations would require are infeasible to create with the current tooling.
Hence, this seminal work will seek to verify the applicability of our compiler based approach on smaller problems, before improving tooling to make larger problems tractable.

Given the use of memory bound problems, testing the performance of kernels within ExaHyPE alone will show us the whole picture.
Therefore, we will also test each kernel by running it numerous times on some fixed test data.
This will mimic what happens within ExaHyPE, but removes all the overhead experienced by the engine, such as memory copies.
This will be our primary metric to compare kernels.
As such we use a small tester program to do this.
This test program creates input data from test cases and creates an output array.
Then the program repeatedly calls the kernel with the same input data and output array, until some time limit is reached e.g. 5 seconds.
We can then use the number of iterations preformed within this time limit to inspect the performance of a kernel.
To ensure the compiler doesn't introduce any unexpected behaviour the test and benchmark orchestration code is compiled without any optimisation flags, whereas all compute kernels are compiled with optimisation flags.

Kernel comparison extend beyond performance, it is also important to validate each of the compiled kernels against the default kernels.
This is achieved by probing the control programs, and extracting input and output data around the default kernel call.
The extracted data is used to create integration tests.
To ensure a variety in the integration tests we extract data when some component of the input state vector is larger than a target value. 
This ensures that our test data wont just be repeats of an empty patch.

Every kernel mentioned in our results underwent validity testing using 10 integration tests and passed each one.

\subsection{Performance of Generated Kernels}
% Default vs generated 
% [ ] kernel compare results 
% [ ] exahype results 

\begin{table}
    \centering
\input{Tables/kernel_compare_results.tex} 
\caption{Performance of compiled vs default kernel}\label{tab:kernel_comapre} 
\end{table}

\subsection{Exploring Generated Kernels Advantage}
% Diving into why - vs hand optimized kernels
% [ ] Introduce hand optimized kernels
% [ ] Results vs hand optimised kernels
% [ ] Talk about compiler flag result

We then explored why compiled kernels have a performance advantage over default kernels.
To do this we focused on the Euler 2D problem, and optimised a kernel by hand.
We will refer to this hand optimised kernel as handmade.
The hand optimised had 3 notable changes over the default kernel.
Firstly, all heap allocation was removed and either completely eliminated or replaced with stack allocation.
Next, all lambda functions were replaced with direct function calls.
And finally, all constants known at compile time (e.g. the number of cells per patch) were removed from function signatures and declared directly in the function body as constants.

Table \ref{tab:hand_v_compiled} shows the relative speedup of the handmade kernels and compiled kernels.

\begin{table}
    \centering
\input{Tables/hand-made-vs-generated-tab.tex} 
\caption{Performance of kernel optimised by hand vs compiled kernel}\label{tab:hand_v_compiled}
\end{table}

During our testing we observed a significant difference

\subsection{Further Optimisations}
% Effect of computation reduction
% [ ] Adding comp dedup speed

\subsection{Compiler Practicalities} \label{sec:practical}
% [ ] Compiler speed
% [ ] Input interface practicalities