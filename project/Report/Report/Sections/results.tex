\subsection{Testing Methodology}
% Test methodology
% [ ] Problems
% [ ] Kernel Validity
% [ ] Exahype results
% [ ] Kernel Compare results

We will use the Euler equations in 2D and 3D, along with the shallow water equations (SWE) to test our compiler. 
A summary of these problems can be found below.

\vspace{1em}
\begin{tabular}{ccccccccc}
Problem & Unknowns+Auxillary & Dim & Patch size & Uses Flux & Uses NCP\\
\hline
Euler 2D & 4+0 & 2 & 3x3 & \checkmark & \xmark \\
Euler 3D & 5+0 & 3 & 3x3x3 & \checkmark & \xmark \\
SWE & 3+1 & 2 & 3x3 & \checkmark & \checkmark \\
\end{tabular}
\vspace{1em}

These problems were chosen as they are prototypical hyperbolic PDE test problems that exhibit a wide range of features, such as 2D and 3D domains and the use of NCP and auxiliary variables.
Also, each of theses problems are provided as example within the ExaHyPE repository.
The kernels from these examples solutions will be treated as a control for the compiled kernels.
We will refer to these kernels as default kernels. 

None of the test problems are compute bound problems, they are all memory bound problems.
Consequently, within an ExaHyPE program we would not expect improving kernel speeds to have a large effect on the total runtime of the program, as the program is bottle necked by memory throughput.

As will be discussed in Section \ref{sec:practical}, creating input DAGs in a non-trivial job.
The size of DAGs that compute bound problems such as the CCZ4 equations would require are infeasible to create with the current tooling.
Hence, this seminal work will seek to verify the applicability of our compiler based approach on smaller problems, before tooling is improved to make larger problems tractable.

Given the use of memory bound problems, only testing the performance of kernels within ExaHyPE will not show us the whole picture.
Therefore, we will also test each kernel in isolation.
This testing will repeatedly run a kernel on fixed input data, allowing us to calculate how many iterations per second a kernel can achieve.
This will mimic what happens within ExaHyPE, but removes all the overhead experienced by the engine, such as memory copies.
This will be our primary metric to compare kernels.

We create a small program, called \texttt{KernelCompare}, to preform this testing.
This test program creates input data from test cases and creates an output array.
Then the program repeatedly calls the kernel with the same input data and output array, until some time limit is reached e.g. 5 seconds.
We can then use the number of iterations preformed within this time limit to inspect the performance of a kernel.
To ensure the compiler doesn't introduce any unexpected behaviour the test and benchmark orchestration code is compiled without any optimisation flags, whereas all compute kernels are compiled with optimisation flags.

Kernel comparison extend beyond performance, it is also important to validate each of the compiled kernels against the default kernels.
This is achieved by probing the control programs, and extracting input and output data around the default kernel call.
The extracted data is used to create integration tests.
To ensure a variety in the integration tests we extract data when some component of the input state vector is larger than a target value. 
This ensures that our test data wont just be repeats of an empty patch.
Every kernel mentioned in our results underwent validity testing using 10 integration tests and passed each one.

Test results are gathered on 2 systems.
An older Intel system running Xeon E5-2680 CPUs.
On this system, unless stated otherwise, all code was compiled with \texttt{icpx} version 2021.1 for the native architecture using the \texttt{-Ofast} optimisation flag.
The other system used is a newer AMD system running EPYC 7702 CPUs.
On this system, unless stated otherwise, all code was compiled with AMD's aocc \texttt{clang++} version 12.0.0 for the native architecture using the \texttt{-Ofast} optimisation flag. 

\subsection{Performance of Generated Kernels}
% Default vs generated 
% [ ] kernel compare results 
% [ ] exahype results 

We tested our compiled kernels against the default kernels in both the \texttt{KernelCompare} tester and within ExaHyPE.
The compiled kernels exhibited an impressive speedup over their default counterparts, as seen in Table \ref{tab:kernel_comapre}.
Over every test we witnessed approximately an order of magnitude speedup of the compiled kernels over the default kernels.
Compiled kernels preformed better on the Intel system than on the AMD system, likely due to differences between the Intel and AMD compilers. 

\begin{table}
    \centering
\input{Tables/kernel_compare_results.tex} 
\caption{Performance of compiled kernels against default kernel in the \texttt{KernelCompare} tester. Speedup is the relative speedup of compiled kernels against their default counterpart.}\label{tab:kernel_comapre} 
\end{table}

In ExaHyPE compiled kernels preformed as expected (Table \ref{tab:exahype}).
We observed a 5\%-15\% speedup in total program runtime, with programs limited by memory throughput.
The Euler 2D kernel improved the least at 5\% and the Euler 3D kernel the most at 15\%.
This is to be expected as values in the Euler 3D patch experience slightly higher arithmetic intensity than those in 2D Euler, hence we can consider Euler 3D slightly less memory bound than Euler 2D.
 
\begin{table}
    \centering
\input{Tables/exahype_results.tex} 
\caption{Performance of compiled kernels against default kernels on the runtime of an ExaHyPE program. Data gathered on the AMD system.}\label{tab:exahype} 
\end{table}

\subsection{Exploring Generated Kernels Advantage}
% Diving into why - vs hand optimized kernels
% [ ] Introduce hand optimized kernels
% [ ] Results vs hand optimised kernels
% [ ] Talk about compiler flag result

We then explored why compiled kernels have a performance advantage over default kernels.
To do this we focused on the Euler 2D problem, and optimised a kernel by hand.
We will refer to this hand optimised kernel as handmade.
The hand optimised had 3 notable changes over the default kernel.
Firstly, all heap allocation was removed and either completely eliminated or replaced with stack allocation.
Next, all lambda functions were replaced with direct function calls.
And finally, all constants known at compile time (e.g. the number of cells per patch) were removed from function signatures and declared directly in the function body as constants.


\begin{table}
    \centering
    \input{Tables/hand-made-vs-generated-tab.tex} 
    \caption{Performance of kernel optimised by hand against compiled kernel. Results gathered on the AMD system.}\label{tab:hand_v_compiled}
\end{table}

Table \ref{tab:hand_v_compiled} shows the relative speedup of the handmade kernels and compiled kernels.
The handmade kernel exhibits a $3.75\times$ speedup over the default kernel, owing mostly to the removal of heap allocation.
However, the compiled kernel exhibits a $2.31\times$ speedup over the handmade kernel.
This shows us that the compiled kernels advantage must gained by other mechanisms beyond the switch to heap allocation.

During our testing we observed a significant difference in the performance of compiled kernels when using the \texttt{-O3} and \texttt{-Ofast} compiler flag on both Intel and AMD compilers.
Exploring this further we found that the \texttt{-ffast-math} was giving us the difference in results.
Table \ref{tab:ffast_math} shows the effect these compiler flags had on the advantage the compiled kernels had.  
\begin{table}
    \centering
\input{Tables/o3_results.tex} 
\caption{}\label{tab:ffast_math} 
\end{table}
\subsection{Further Optimisations}
% Effect of computation reduction
% [ ] Adding comp dedup speed


\subsection{Compiler Practicalities} \label{sec:practical}
% [ ] Compiler speed
% [ ] Input interface practicalities