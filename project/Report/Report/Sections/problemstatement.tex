\subsection{Background}
Our focus will be applying the Finite Volume (FV) scheme to solve linear and non-linear hyperbolic PDEs in first order form.
We express these PDEs mathematically with the equation
% See exahype template
\begin{equation}\label{eq:non-lin-pde}
    \frac{\partial \mathbf{Q}}{\partial t}(x,t) + \nabla \cdot \mathbf{F}(\mathbf{Q}) + \mathbf{B}(\mathbf{Q}) \cdot \nabla \mathbf{Q}(x,t) = \mathbf{S}(\mathbf{Q})
\end{equation}
where $\mathbf{Q}(x,t)\subset \mathbb{R}^q$ is a time and space dependent state vector for any $x$ in our spatial domain $\Omega\subset \mathbb{R}^{d \in \{2,3\}}$ and $t>0$.
The number of conserved state variables, $q$, can vary from $5$ in the 3D Euler Equations, $58$ in the Einstien Relativity equations (CCZ4) \cite{CCZ4}.
$\mathbf{F}$ is the conserved flux vector, $\mathbf{B}$ the (system) matrix composing the non-conservative fluxes and $\mathbf{S}(\mathbf{Q})$ the source term.
The $\mathbf{B}(\mathbf{Q}) \cdot \nabla \mathbf{Q}(x,t)$ term is often called the NCP (non-conservative product).


We will derive the FV scheme for  (\ref{eq:non-lin-pde}) in the case that $B(Q)=S(Q)=0$
\begin{equation*}
    \frac{\partial \mathbf{Q}}{\partial t} + \nabla\cdot \mathbf{F}(\mathbf{Q}) = 0.
\end{equation*}
FV begins by dividing the spacial domain into finite volumes, known as cells.
For a cell, $i$, we take an integral over its volume $v_i$
\begin{equation*}
    \int_{v_i}\frac{\partial \mathbf{Q}}{\partial t}\,dv + \int_{v_i}\nabla\cdot \mathbf{F}(\mathbf{Q})\,dv = 0.
\end{equation*}
Then we integrate the first term to give the average state $\bar{\mathbf{Q}_i}$ over the cell, and apply divergence theorem to the second term
\begin{equation}\label{eq:fv-almost-done}
    v_i\frac{\partial \bar{\mathbf{Q}_i}}{\partial t} + \oint_{S_i}\mathbf{F}(\mathbf{Q})\cdot \mathbf{n} \, dS = 0
\end{equation}
where $S_i$ is the surface of cell $i$ and $\mathbf{n}$ is the outward facing normal vector.
Rearranging (\ref{eq:fv-almost-done}) for the time derivative we find
\begin{equation}\label{eq:fv-done}
   \frac{\partial \bar{\mathbf{Q}_i}}{\partial t} = -\frac{1}{v_i} \oint_{S_i}\mathbf{F}(\mathbf{Q})\cdot \mathbf{n} \, dS.
\end{equation}

If we select an simple geometry for our cell, squares/cubes are used in ExaHyPE, then calculating the 2D surface integral is simply a sum across the 4 faces of a square (or in 3D, the 6 faces of a cube).
Asserting $S_i$ is a finite set of faces allows us to simply the surface integral
\begin{equation}\label{eq:fv-simple}
   \frac{\partial \bar{\mathbf{Q}_i}}{\partial t} = -\frac{1}{v_i} \sum_{S_i}\mathbf{F}(\mathbf{Q})\cdot \mathbf{n}.
\end{equation}
This provides a method to calculate the time derivative of $Q$, which can be used in a time stepping scheme such as explicit Euler to progress $Q$.


\subsection{ExaHyPE}
\newcommand{\proc}[1]{\textit{#1}}
Algorithmically a FV cell is an array of \texttt{double}.
In ExaHyPE cells are grouped up into patches, commonly $3\times 3$ cells in 2D or $3\times 3 \times 3$ cells in 3D, and these patches are stored as an AoS (Array of Structures).
The Peano framework within ExaHyPE is responsible for distributing these patches across processing cores and nodes, where they are updated using (\ref{eq:fv-simple}) and an explicit euler time stepping scheme, by a single core.
We will refer to this update process as \proc{Patch Update}.
As an aside, in (\ref{eq:fv-simple}) we set $B(Q)=S(Q)=0$ for simplicity, however in the case $B(Q), S(Q)\neq 0$ the update process is broadly similar to (\ref{eq:fv-simple}), the main feature still being a sum across the faces of a cell.

An issue encountered in (\ref{eq:fv-simple}) is: what is the value of $Q$ at a face?
Take a the face between cells $\alpha$ and $\beta$.
We know the average state across $\alpha$ which we call $Q_\alpha$, and likewise for $\beta$ with average state $Q_\beta$.
However the face, $f$, lies on the boundary of $\alpha$ and $\beta$, and its sate $Q_f$ is at discontinuity between $Q_\alpha$ and $Q_\beta$, so what should $Q_f$ be?
This is know as the Riemann problems.
Simple solutions exist, such as taking an average $Q_f = \frac{Q_\alpha + Q_\beta}{2}$, and the best choice of scheme is problem specific.
By default, ExaHyPE uses the Rusanov scheme \cite{rusanov} as a general purpose solution to the Riemann problem, however user have the option to provide their own scheme.
Once a value of $Q_f$ has been decided upon then $F(Q_f)\cdot \mathbf{n}$ (and additionally NCP terms if applicable) can be calculated.
We call this process the \proc{Numerical Ingredient}, and we can trace it back to the evaluation of $F(Q)\cdot\mathbf{n}$ in (\ref{eq:fv-simple}).

Adding further complexity to \proc{Numerical Ingredient}, it is not always the case that we want to model the face between $\alpha$ and $\beta$ as a shared face.
Sometimes we treat $f$ as two faces $f_\alpha, f_\beta$, which means we cannot assume that evaluating the \proc{Numerical Ingredient} while updating $\alpha$ will be the same as when updating $\beta$.
The Rusanov scheme used by default in ExaHyPE doesn't experience this complication, treating the face as a shared face. 
Thus, this is a good example of ExaHyPE supporting functionality which is not required by default, but may be required for certain problems.
And, this support adds additional barriers to optimisation.

Referring back to (\ref{eq:fv-simple}), a user must provide code to calculate $F(Q)\cdot\mathbf{n}$, which we will call one of the \proc{Problem Descriptions}.
If their equation has a non-zero source term and NCP term a user will also need to provide code to calculate these quantities.
Additionally Rusanov requires the max-eigenvalues of a cell, so this must also be provided by the user.
Finally a user will need to provide code that describes the boundary conditions of the problem.
This is all the code a user needs to provide.

These 3 process are split into 3 functions in ExaHyPE, a summary of the number of inputs and outputs of these functions can be found in Table \ref{tab:patch_update}.

\begin{table}
\begin{tabular}{lllcc}
    \toprule
    Process & User/Engine &Uses Functions & Input Size & Output Size\\
    \midrule
    \proc{Patch Update} (\proc{PU})&Engine& \makecell[l]{\proc{NI}, \proc{PD}\\ (source term)} & $q \cdot (p+2)^d+m$ & $q\cdot p^d$\\
    \proc{Numerical Ingredient} (\proc{NI}) &\makecell[l]{Engine \\(default),\\ User}& \makecell[l]{\proc{PD} (flux, ncp,\\ max eigenvalues)} & $2q+m$ & $2q$\\
    \proc{Problem Descriptions} (\proc{PD}) & User& - & $q+m$ & $q$ or $1$\\
    \bottomrule
\end{tabular}
\caption{A summary of the level of abstraction within the \proc{Patch Update} function. $p$ is the number of cells along 1 dimension in a patch, $d$ is the dimension of the patch, $q$ are the number of state variables, $m$ is the size of patch/state metadata.}\label{tab:patch_update}
\end{table}

ExaHyPE uses a functional paradigm to implement these 3 groups of functions.
An overview is given below:
\begin{lstlisting}[language=c]
void numerical_ingredient(std::function<...> user_flux, ...){
    // ... Do something
    flux = user_flux(...);
    // ... Use the value of flux and ncp (if applicable) 
};

void patch_update(std::function<...> numerical_ingredient, ...){
    // ... Do something
    face_value = numerical_ingredient(...);
    // ... Use the value of numerical ingredient
};

void generated_glue_code(){
    auto user_flux = project::user_land::flux;
    auto numerical_ingredient_lambda = [&](...){numerical_ingredient(user_flux, ...)};
    patch_update(numerical_ingredient_lambda, ...);
};
\end{lstlisting}
This patten completely decouples engine functions like \texttt{patch\_update} and \texttt{rusanov} from user functions, hence enabling support of any user \proc{Problem Definitions} provided they implement the required function signatures.
This paradigm helps ExaHyPE achieve its goals of being flexible and user friendly but introduces some pitfalls for the compiler that may hinder optimisation.

Firstly, it is non-trivial to inline this paradigm.
As such, there may be function calls in our final assembly, which limits the amount of SIMD vectorization the compiler can preform.
This is because function calls may overwrite SIMD registers, so the compiler must treat the contents of a SIMD register as unknown after a function call, ultimately stifling vectorization.

The next issue with this paradigm is it obscures the existence of unused and repeated computation.
Metadata, such as the current time and timestep size, is passed through all functions to bottom level \proc{Problem Definition} functions.
Some of this metadata is repeatedly calculated e.g. the spacial coordinate of the face being processed by the \proc{Numerical Ingredient}.
Although some \proc{Numerical Ingredients} and \proc{Problem Definitions} may use this calcualted metadata, many don't.
As this calculation and its potential use lies across function calls it is possible the compiler is unable to remove this redundant code.

Beyond unused computation, there can also be repeated computation.
One such example is when we use Rusanov as our \proc{Numerical Ingredient}.
To calculate the change of state at the shared face of 2 cells $\alpha,\beta$ Rusanov begins by calculating $flux(\alpha)$, $flux(\beta)$, $max\_eigenvalue(\alpha)$, $max\_eigenvalue(\beta)$.
Then using these values the change of state is calculated.
If we then calculate the change of state between $\beta,\gamma$ note that the computation of $flux(\beta)$, $max\_eigenvalue(\beta)$ is repeated.
It may be challenging for the compiler to spot this repetition, especially as it again lies across multiple different functions.
Furthermore, \proc{Patch Update} cannot be optimised to take advantage of this property of Runsnov, as it would no longer work with any \proc{Numerical Ingredient}.


% ISSUE: heap allocation
The final issue we will highlight is the use of heap allocation.
User's problems can have an arbitrary number of unknowns and auxillary variables, 
as such we cannot assume it is safe to allocate temporary state variables on the stack.
We must instead initiate temporary state variables on the heap, which is slow.
Both \proc{Patch Update} and \proc{Numerical Ingredient} need to create these temporary variables, which leads to a lot of heap allocation.
This is one of the major downsides of engine code not knowing anything about use code, we must implement defensive engine code.

% ISSUE: branching
% MAYBE ISSUE: consts as function arguments

This is not an exhaustive list of potential performance issues currently experience in ExaHyPE \proc{Patch Update} function.
But it does highlight some of the issues that static code generation faces while having to operate safely across a wide domain of user's problem.
\phlat aims to employ dynamic code generation to maintain the flexibility currently afforded in ExaHyPE, while allowing more aggressive optimisation to take place.
