% Can we make exahype better
% while not making it worse
\newcommand{\proc}[1]{\textbf{#1}}

% we will focus on patch update
An ExaHyPE project is a majority of engine code, and a small amount of user code which is used to describe the PDE.
The interface between the engine and user code is built on the patten of passing function pointers of user functions to engine code.
Here is an example:
\begin{lstlisting}[language=c]
void engine_function(std::function<...> user_function){
    // ... Do something
    val = user_function();
    // ... Use the value of the user function 
}

void engine_core(){
    auto user_func = project::user_land::foo;
    engine_function(user_func);
}
\end{lstlisting}

This patten is good design practice, as it completely decouples engine functions from user functions, apart from at a single crossover point.
Thereby allowing the engine to be developed in isolation from user codes, and allowing the engine to support any user codes that respect the required function signatures.
However, this patten is not without its disadvantages, as we will discuss.

Our focus will be on the \proc{Patch Update} process.
This process is heavily reliant on user code, and as such dominates the runtime of user code.
Furthermore, for arithmetic intense PDEs, this process dominates the entire running time of an ExaHyPE program.
We will begin by explaining the \proc{Patch Update} process.
Then proceed to highlight inefficiencies within this process, many of which are artifacts of the aforementioned function pointer patten. 

The smallest unit of space in a FV scheme is a cell.
Algorithmically a cell is an array of \lstinline{double} that store the state of that cell.
As mentioned previously the size of the state vector can vary from $4$ values to over $40$.
Cells are then grouped up into a patch.
Patch sizes are typically 3x3 cells in 2D or 3x3x3 cells in 3D.
The size of patches can be adjusted by a user, say to 5x5, however a patch size is a constant throughout the runtime of the program.
\proc{Patch Update} is used to evolve the values of all the cells in a patch by a given timestep.
This is done by applying source terms to the cells, then calculating flux and non-conservative products (NCP) for every face of a cell and applying it to the cells state.
As a technical detail, to evaluate every face of every cell in a patch we require a halo of cells as demonstrated in Figure [TODO].
Within ExaHyPE, patch data is stored in a AoS (Array of Structures) form.
In summary, \proc{Patch Update} is a function whose input is: a haloed patch; a timestep; and patch meta-data such as its size in space. And whose output is: the patch (without a halo) that has been progressed in time. 

Calculating the flux and NCP at a shared face between 2 cells a non-trivial task.
This is because the face is at a discontinuatly between the states of the 2 cells, and is known as the Riemann problem in FV.
As such, we require a scheme to solve the Riemann problem, which we will call the \proc{Numerical Ingredient}.
Many such schemes exist, such as Upwind and Downwind schemes, and the correct scheme to depends on the problem at hand.
Within ExaHyPE the Rusanov scheme \cite{rusanov} is used by default, as it well serves the role of a general \proc{Numerical Ingredient}.
The \proc{Numerical Ingredient} has an input of 2 cells and meta data about the face (e.g. its position in space).
And outputs 2 state vectors describing the change of state experienced by each cell.

At the bottom level of this hierachy we have the \proc{Problem Descriptions}.
These are a collection of user defined functions the calculate: flux, ncp, source terms, and maximum eigen values.
All this functions share a similar input interface, taking single cell and the meta data for that cell.
And they output either a state vector or single value, depending on what quantity they are calculating.
In the default ExaHyPE setup, it will be \proc{Patch Update} that calls the source term function.
And \proc{Numerical Ingredient} that calls the ncp, flux and max eigen value functions.

A summary of the abstraction hierachy of \proc{Patch Update} can be found in Table \ref{tab:patch_update}

\begin{table}
\begin{tabular}{llcc}
    \toprule
    Process & Uses Functions & Input Size & Output Size\\
    \midrule
    \proc{Patch Update} & \makecell[l]{\proc{Numerical Ingredient},\\ \proc{Problem Descriptions}\\ (source term)} & $q \cdot (p+2)^d+m$ & $q\cdot p^d$\\
    \proc{Numerical Ingredient} & \makecell[l]{\proc{Problem Descriptions} \\ (flux, ncp, max eigenvalues)} & $2q+m$ & $2q$\\
    \proc{Problem Descriptions} & - & $q+m$ & $q$ or $1$\\
    \bottomrule
\end{tabular}
\caption{A summary of the level of abstraction within the \proc{Patch Update} function. $p$ is the number of cells along 1 dimension in a patch, $d$ is the dimension of the patch, $q$ are the number of state variables, $m$ is the size of patch/state metadata.}\label{tab:patch_update}
\end{table}

\newcommand{\func}[1]{\textbf{#1}}
\newcommand{\var}[1]{\textbf{#1}}


The flexible nature of ExaHyPE, along with its use of template driven code generation, experiences performance issues at the interface between the user and engine code.
Namely, engine code at this interface has to be general to accept a large range user codes, however this comes at the cost of performance.


Next we will take an indepth look at where there are potential inefficiencies within the Path Update process.
We will look at the 2D Euler equations and use the engine provided Rusanov numerical method.


\func{Patch update} starts by creating 2 variables \var{tmpA}, \var{tmpB} which are vectors an equal same size as a cell.

\begin{lstlisting}[language=c]
double * tmpA = ::tarch::allocateMemory(unknowns, tarch::MemoryLocation::Heap);
double * tmpB = ::tarch::allocateMemory(unknowns, tarch::MemoryLocation::Heap);
\end{lstlisting}

These variables are used to store the return values from calls to the source term function and \func{Numerical Method} steps.
To provide safe behaviour for any possible problem, these variables are allocated on the heap.
For many problems, including our example 2D Euler problem, cell sizes are small (e.g. 4 doubles), hence heap allocation is not required. 
This is our first definite inefficiency.
This issue also appears again in the Numerical method step which is used many times in \func{Patch update}.

The next step in \func{Patch update} iterates through every cell in the patch, applying the source term.
\begin{lstlisting}[language=c]
for(int i=0; i<patchSize; i++){
    for(int j=0; j<patchSize; j++){
        double * cellData = ...
        double cellCenterX = ...
        double cellCenterY = ...    
        sourceTerm(cellData, cellCenterX, cellCenterY, ..., tmpA);
        for(int k=0; k<unknowns; k++){
            outCell[...][k] = tmpA[k] * ...
        }
    }
}
\end{lstlisting}


We can observe that these loops are of fixed size, as the patchSize is known at compile time. 
Hence, they can be unrolled and values such as cellData can be precomputed.   
However, patchSize is an argument to patchUpdate and then patch update wrapped in a lambda function.
Therefore we run the risk of our compiler being unable to spot this optimisation.
We will label this a possible inefficiency.

This section of code also exhibits a recurring patten of function calls to lambda functions, receiving return values in one of our temporary variables.
In this case it is likely that the compiler will inline the code.
However proceeding calls to nested functions such as \func{Numerical Method} are much harder to inline, hence risk limiting the compilers ability to do SIMD operations.    
This is a second possible inefficiency.

After we process the source terms we update each cell by evaluating the flux between cell faces.
This also uses the double for loop patten in example (REF). %TODO
This step calls our numerical method on every pair on neighboring cells which we will call left and right.
In our example we use Rusanov as our numerical method.
In Rusanov we calculate the eigenvalues for the left cell and the right cell.
However, if we think about the traversal of cells e.g. [(0,0), (0,1)] [(0,1), (0,2)], .... First (0,1) will be a right cell, and then in the next call to Rusanov it will be the left cell, hence we calculate with eigenvalue for (0,1) twice, even though it hasn't changed.
This is our 2nd definite inefficiency.
This issue eludes to a set of definite inefficiencies where there are inter-process optimisation opportunities (e.g. patch update and numerical method can work together to avoid this double eigenvalue issue).   


Looking at the inefficiencies we identified we can see that they all stem from the general interface between user and engine code.
Given time, it is possible for a developer to manually fix these issues.
For example we took our example and fixed all issues excluding the double eigenvalue issue.
Doing this we observed a 3x speedup in the runtime of patch update, resulting in a 10\% speedup of the full program.

However these optimisation require a significant amount of manual work to preform, while risking both the introduction of errors and a reduction of readability/usability of users codes.

Therefore we propose creating a domain specific compiler that are make these optimisations automatically.


