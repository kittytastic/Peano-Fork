#include "tarch/Assertions.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/datatraversal/TaskSet.h"
#include "peano/performanceanalysis/ScorePMacros.h"
#include "tarch/multicore/RecursiveLock.h"
#include "tarch/multicore/MulticoreDefinitions.h"
#include "tarch/services/Service.h"


template<class Data, class SendReceiveTaskType, class VectorContainer>
tarch::logging::Log  peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::_log( "peano::heap::BoundaryDataExchanger" );


template<class Data, class SendReceiveTaskType, class VectorContainer>
peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::BoundaryDataExchanger():
  _identifier( "created-by-standard-constructor"),
  _metaDataTag(-1),
  _dataTag(-1),
  _rank(-1),
  _numberOfSentMessages(-1),
  _numberOfSentRecords(-1),
  _numberOfReceivedMessages(-1),
  _numberOfReceivedRecords(-1),
  _currentReceiveBuffer(-1),
  _readDeployBufferInReverseOrder(false),
  _wasTraversalInvertedThroughoutLastSendReceiveTraversal(false)
  #ifdef Asserts
  ,_isCurrentlySending(false)
  #endif
{
  #if defined(MPIHeapUsesItsOwnThread) and defined(MultipleThreadsMayTriggerMPICalls) and defined(SharedMemoryParallelisation)
  _backgroundThread = nullptr;
  #endif

  assertion5( _sendTasks.empty(),       _sendTasks.size(),       _identifier, _metaDataTag, _dataTag, _rank );
  assertion5( _receiveTasks[0].empty(), _receiveTasks[0].size(), _identifier, _metaDataTag, _dataTag, _rank );
  assertion5( _receiveTasks[1].empty(), _receiveTasks[1].size(), _identifier, _metaDataTag, _dataTag, _rank );
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::BoundaryDataExchanger(
  const std::string& identifier,
  int metaDataTag,
  int dataTag,
  int rank
):
  _identifier(identifier),
  _metaDataTag(metaDataTag),
  _dataTag(dataTag),
  _rank(rank),
  _numberOfSentMessages(0),
  _numberOfSentRecords(0),
  _numberOfReceivedMessages(0),
  _numberOfReceivedRecords(0),
  _currentReceiveBuffer(0),
  _readDeployBufferInReverseOrder(false),
  _wasTraversalInvertedThroughoutLastSendReceiveTraversal(false)
  #ifdef Asserts
  ,_isCurrentlySending(false)
  #endif
  {
  #if defined(MPIHeapUsesItsOwnThread) and defined(MultipleThreadsMayTriggerMPICalls) and defined(SharedMemoryParallelisation)
  _backgroundThread = nullptr;
  #endif

  assertion5( _sendTasks.empty(),       _sendTasks.size(),       _identifier, _metaDataTag, _dataTag, _rank );
  assertion5( _receiveTasks[0].empty(), _receiveTasks[0].size(), _identifier, _metaDataTag, _dataTag, _rank );
  assertion5( _receiveTasks[1].empty(), _receiveTasks[1].size(), _identifier, _metaDataTag, _dataTag, _rank );
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::shutdown() {
  #if defined(MPIHeapUsesItsOwnThread) and defined(MultipleThreadsMayTriggerMPICalls) and defined(SharedMemoryParallelisation)
  if (_backgroundThread != nullptr) {
    _backgroundThread->terminate();
    _backgroundThread = nullptr;
  }
  assertion( _backgroundThread == nullptr );
  #endif

  assertion5( _sendTasks.empty(),       _sendTasks.size(),       _identifier, _metaDataTag, _dataTag, _rank );
  assertion5( _receiveTasks[0].empty(), _receiveTasks[0].size(), _identifier, _metaDataTag, _dataTag, _rank );
  assertion5( _receiveTasks[1].empty(), _receiveTasks[1].size(), _identifier, _metaDataTag, _dataTag, _rank );
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::~BoundaryDataExchanger() {
  shutdown();
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::startToSendData(bool isTraversalInverted) {
  logTraceInWith1Argument( "startToSendData(bool)", isTraversalInverted );

  tarch::multicore::RecursiveLock lock( tarch::services::Service::receiveDanglingMessagesSemaphore );

  assertion(_currentReceiveBuffer>=0);
  assertion(_currentReceiveBuffer<=1);
  assertionMsg(_receiveTasks[1-_currentReceiveBuffer].empty(), "finishedToSendData() has not been called before and deploy buffer thus is not empty. Check whether allHeapsFinishedToSendBoundaryData() on AbstractHeap has been invoked or your mappings delegate heap management to kernel. If finish is called, this error pops up if we you don't pick up messages sent to a rank in the iteration before");

  assertion3(!_isCurrentlySending, _identifier, _rank, "call once per traversal" );

  #ifdef Asserts
  _isCurrentlySending = true;
  #endif

  const int numberOfSentMessages = getNumberOfSentMessages();

  releaseSentMessages();

  waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages(numberOfSentMessages);

  releaseReceivedNeighbourMessagesRequests();

  assertion3(
    _receiveTasks[0].empty() || _receiveTasks[1].empty(),
    _receiveTasks[0].size(),
    _receiveTasks[1].size(),
    _currentReceiveBuffer
  );

  _readDeployBufferInReverseOrder = _wasTraversalInvertedThroughoutLastSendReceiveTraversal != isTraversalInverted;

  switchReceiveAndDeployBuffer(numberOfSentMessages);

  postprocessStartToSendData();

  logTraceOut( "startToSendData(bool)" );
}



template <class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages(
  int numberOfMessagesSentThisIteration
) {
  logTraceInWith2Arguments( "waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages()", _rank, numberOfMessagesSentThisIteration );

  const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
  const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
  bool           triggeredTimeoutWarning = false;

  while (static_cast<int>(_receiveTasks[_currentReceiveBuffer].size()) < numberOfMessagesSentThisIteration ) {
    if (
       tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
       (clock()>timeOutWarning) &&
       (!triggeredTimeoutWarning)
    ) {
       tarch::parallel::Node::getInstance().writeTimeOutWarning(
         "peano::heap::BoundaryDataExchanger::waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages",
         "waitUntilNumberOfReceivedMessagesEqualsNumberOfSentMessages()", _rank,
         _metaDataTag, numberOfMessagesSentThisIteration - _receiveTasks[_currentReceiveBuffer].size()
       );

       triggeredTimeoutWarning = true;
    }
    if (
       tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
       (clock()>timeOutShutdown)
    ) {
       tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
         "peano::heap::BoundaryDataExchanger::waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages",
         "waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages()", _rank,
         _metaDataTag, numberOfMessagesSentThisIteration - _receiveTasks[_currentReceiveBuffer].size()
       );
    }

    tarch::parallel::Node::getInstance().receiveDanglingMessages();
  }

  logTraceOutWith2Arguments( "waitUntilNumberOfReceivedNeighbourMessagesEqualsNumberOfSentMessages()", _identifier, _receiveTasks[_currentReceiveBuffer].size() );
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::releaseSentMessages() {
  logTraceInWith1Argument( "releaseSentMessages()", _sendTasks.size() );

  logInfo( "releaseSentMessages()", "release the " << _sendTasks.size() << " send task(s)" );

  for(typename std::list<SendReceiveTaskType >::iterator i = _sendTasks.begin(); i != _sendTasks.end(); ++i) {
    const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
    const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
    bool           triggeredTimeoutWarning = false;
    int            finishedWait            = i->_metaInformation.getLength()==0;

    while (!finishedWait) {
      assertion1(i->_data!=nullptr, i->toString() );
      MPI_Test(&(i->_request), &finishedWait, MPI_STATUS_IGNORE);

      // deadlock aspect
      if (
         tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
         (clock()>timeOutWarning) &&
         (!triggeredTimeoutWarning)
      ) {
         tarch::parallel::Node::getInstance().writeTimeOutWarning(
           "peano::heap::SynchronousDataExchanger",
           "finishedToSendData()", i->_rank,-1,i->_metaInformation.getLength()
         );
         triggeredTimeoutWarning = true;
      }
      if (
         tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
         (clock()>timeOutShutdown)
      ) {
         std::ostringstream msg;
         msg << "metaDataTag=" << _metaDataTag;
         msg << ",dataTag=" << _dataTag;
         tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
           "peano::heap::SynchronousDataExchanger",
           "finishedToSendData()", i->_rank,
           -1,
           i->_metaInformation.getLength(), msg.str()
         );
      }
      tarch::parallel::Node::getInstance().receiveDanglingMessages();
    }

    i->_metaInformation.setLength(0);
    i->freeMemory();
  }

  _sendTasks.clear();

  logTraceOutWith2Arguments( "releaseSentMessages()", _numberOfSentMessages, _numberOfSentRecords );
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::finishedToSendData(bool isTraversalInverted) {
  logTraceInWith1Argument( "finishedToSendData(bool)", isTraversalInverted );

  assertion3(_isCurrentlySending, _identifier, _rank, "call once per traversal" );
  #ifdef Asserts
  _isCurrentlySending = false;
  #endif

  _wasTraversalInvertedThroughoutLastSendReceiveTraversal = isTraversalInverted;

  tarch::multicore::RecursiveLock lock( tarch::services::Service::receiveDanglingMessagesSemaphore );
  postprocessFinishedToSendData();

  logTraceOut( "finishedToSendData(bool)" );
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::receiveDanglingMessages() {
  #if defined(MPIHeapUsesItsOwnThread) and defined(MultipleThreadsMayTriggerMPICalls) and defined(SharedMemoryParallelisation)
  if (_backgroundThread==nullptr) {
    _backgroundThread = new BackgroundThread(this);
    peano::datatraversal::TaskSet spawnTask(_backgroundThread,peano::datatraversal::TaskSet::TaskType::BackgroundMPIReceiveTask);
  }
  assertion( _backgroundThread != nullptr );
  #endif

  SCOREP_USER_REGION("peano::heap::BoundaryDataExchanger::receiveDanglingMessages()", SCOREP_USER_REGION_TYPE_FUNCTION)

  tarch::multicore::RecursiveLock lock( tarch::services::Service::receiveDanglingMessagesSemaphore );

  #ifdef MPIProgressionReliesOnMPITest
  int progressFlag;
  for(auto& p: _sendTasks) {
    if(p._metaInformation.getLength() > 0) {
      MPI_Test(&(p._request), &progressFlag, MPI_STATUS_IGNORE);
      if (progressFlag) {
        p._metaInformation.setLength(0);
        p.freeMemory();
      }
    }
  }
  for(auto& p: _receiveTasks[_currentReceiveBuffer]) {
    if(p._metaInformation.getLength() > 0) {
      MPI_Test(&(p._request), &progressFlag, MPI_STATUS_IGNORE);
      if (progressFlag) {
        p._metaInformation.setLength(0);
      }
    }
  }
  #endif


  int        flag   = 1;
  while (flag) {
    int  result = MPI_Iprobe(
      _rank,
      _metaDataTag,
      tarch::parallel::Node::getInstance().getCommunicator(),
      &flag, MPI_STATUS_IGNORE
    );
    if (result!=MPI_SUCCESS) {
      logError(
        "receiveDanglingMessages()",
        "probing for messages failed: " << tarch::parallel::MPIReturnValueToString(result)
      );
    }
    if (flag) {
      logTraceInWith1Argument( "receiveDanglingMessages(...)", _metaDataTag );

      SendReceiveTaskType receiveTask;

      const int receiveMetaDataMode = -1;
      receiveTask._metaInformation.receive(_rank, _metaDataTag, true, receiveMetaDataMode);
      receiveTask._rank = _rank;

      _numberOfReceivedMessages += 1;
      _numberOfSentRecords      += receiveTask._metaInformation.getLength();

      handleAndQueueReceivedTask( receiveTask );

      logTraceOutWith1Argument( "receiveDanglingMessages(...)", receiveTask._metaInformation.toString() );
    }
  }
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::sendData(
  const Data* const                                    data,
  int                                                  count,
  const tarch::la::Vector<DIMENSIONS, double>&         position,
  int                                                  level
) {
  assertion5( _isCurrentlySending, _identifier, count, position, level, "forgot to call startToSendBoundaryData on heap?" );

  SendReceiveTaskType sendTask;
  sendTask._rank = _rank;
  #ifdef Asserts
  //Set debug information
  sendTask._metaInformation.setPosition(position);
  sendTask._metaInformation.setLevel(level);
  #endif

  logDebug("sendData", "sending data at " << position << " to Rank " << _rank << " with tag " << _dataTag  );

  sendTask._metaInformation.setLength(count);

  tarch::multicore::RecursiveLock lock( tarch::services::Service::receiveDanglingMessagesSemaphore );
  handleAndQueueSendTask( sendTask, data );

  _numberOfSentMessages += 1;
  _numberOfSentRecords  += count;
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
VectorContainer  peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::receiveData(
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level
) {
  logTraceInWith4Arguments( "receiveData(...)", _identifier, _rank, position, level );

  assertion( _currentReceiveBuffer>=0 );
  assertion( _currentReceiveBuffer<=1 );

  const int currentDeployBuffer = 1-_currentReceiveBuffer;
  typename std::list<SendReceiveTaskType >::iterator readElement = _readDeployBufferInReverseOrder ? --_receiveTasks[currentDeployBuffer].end() : _receiveTasks[currentDeployBuffer].begin();

  assertion2( _receiveTasks[currentDeployBuffer].size()>0, tarch::parallel::Node::getInstance().getRank(), "if the neighbour data buffer is empty, you have perhaps forgotten to call releaseMessages() on the heap in the traversal before" );

  #ifdef Asserts
  if ( heapStoresVertexSpatialDataForValidation() ) {
    assertion10(
      readElement->fits(position,level) | !dataExchangerCommunicatesInBackground(),
      _readDeployBufferInReverseOrder,
      level,  position,
      _receiveTasks[currentDeployBuffer].front()._metaInformation.toString(),
      _receiveTasks[currentDeployBuffer].back()._metaInformation.toString(),
      tarch::parallel::Node::getInstance().getRank(),
      _rank,
      _identifier,
      dataExchangerCommunicatesInBackground(),
      "if _readDeployBufferInReverseOrder: take back() element"
    );

    const int numberOfElementsOfThisEntry = readElement->_metaInformation.getLength();
    assertion3(numberOfElementsOfThisEntry >= 0, position, level, numberOfElementsOfThisEntry);
    assertion3(readElement->_data!=0 || numberOfElementsOfThisEntry==0, position, level, numberOfElementsOfThisEntry);
  }
  #endif

  const VectorContainer result(readElement->_data, readElement->_data+readElement->_metaInformation.getLength());

  readElement->freeMemory();
  _receiveTasks[currentDeployBuffer].erase(readElement);

  logTraceOutWith1Argument( "receiveData(...)", result.size() );
  return result;
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::plotStatistics() const {
  logInfo(
    "plotStatistics()",
    "records sent by " << _identifier << " to " << _rank << " on tag " << _metaDataTag << ": " << _numberOfSentRecords
  );

  logInfo(
    "plotStatistics()",
    "messages sent by " << _identifier << " to " << _rank << " on tag " << _metaDataTag << ": " << _numberOfSentMessages
  );

  logInfo(
    "plotStatistics()",
    "records received by " << _identifier << " to " << _rank << " on tag " << _metaDataTag << ": " << _numberOfReceivedRecords
  );

  logInfo(
    "plotStatistics()",
    "messages received by " << _identifier << " to " << _rank << " on tag " << _metaDataTag << ": " << _numberOfReceivedMessages
  );

  logInfo(
    "plotStatistics()",
    "current send queue of " << _identifier << " holds " << _sendTasks.size() << " message(s)"
  );

  logInfo(
    "plotStatistics()",
    "current receive buffer of " << _identifier << " holds " << _receiveTasks[_currentReceiveBuffer].size() << " message(s)"
  );

  logInfo(
    "plotStatistics()",
    "current deploy buffer of " << _identifier << " holds " << _receiveTasks[1-_currentReceiveBuffer].size() << " message(s)"
  );
}


template<class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::clearStatistics() {
  _numberOfSentMessages     = 0;
  _numberOfSentRecords      = 0;
  _numberOfReceivedMessages = 0;
  _numberOfReceivedRecords  = 0;
}


template <class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::releaseReceivedNeighbourMessagesRequests() {
  logTraceIn( "releaseReceivedNeighbourMessagesRequests()" );

  bool allMessageCommunicationsAreFinished = !dataExchangerCommunicatesInBackground();

  const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
  const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
  bool           triggeredTimeoutWarning = false;
  int            finishedWait            = false;

  logInfo(
    "releaseReceivedNeighbourMessagesRequests()",
	"check all " << _receiveTasks[_currentReceiveBuffer].size() << " messages to be received from rank " << _rank
  );
  while (!allMessageCommunicationsAreFinished) {
    allMessageCommunicationsAreFinished = true;

    logDebug( "releaseReceivedNeighbourMessagesRequests()", "check all " << _receiveTasks[_currentReceiveBuffer].size() << " messages from rank " << _rank );

    for(
      typename std::list<SendReceiveTaskType >::iterator i = _receiveTasks[_currentReceiveBuffer].begin();
      i != _receiveTasks[_currentReceiveBuffer].end();
      i++
    ) {
      if(i->_metaInformation.getLength() > 0) {
        MPI_Test(&(i->_request), &finishedWait, MPI_STATUS_IGNORE);
        allMessageCommunicationsAreFinished &= (finishedWait!=0);
      }
    }

    // deadlock aspect
    if (
       tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
       (clock()>timeOutWarning) &&
       (!triggeredTimeoutWarning)
    ) {
       tarch::parallel::Node::getInstance().writeTimeOutWarning(
         "peano::heap::BoundaryDataExchanger",
         "releaseReceivedMessagesRequests()", -1,
         _metaDataTag, -1
       );
       triggeredTimeoutWarning = true;
    }
    if (
       tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
       (clock()>timeOutShutdown)
    ) {
       tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
         "peano::heap::BoundaryDataExchanger",
         "releaseReceivedMessagesRequests()", -1,
         _metaDataTag, -1
       );
    }

    // @see Remark on overtaking messages in switchReceiveAndDeployBuffer()
    //      It explains that this line has to be commented out.
    // tarch::parallel::Node::getInstance().receiveDanglingMessages();
  }

  logTraceOut( "releaseReceivedNeighbourMessagesRequests()" );
}


template <class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::switchReceiveAndDeployBuffer(int numberOfMessagesSentThisIteration) {
  assertion(_receiveTasks[1-_currentReceiveBuffer].empty());

  tarch::multicore::RecursiveLock lock( tarch::services::Service::receiveDanglingMessagesSemaphore );

  _currentReceiveBuffer = 1-_currentReceiveBuffer;

  const int sizeOfNewDeployBuffer = static_cast<int>(_receiveTasks[1-_currentReceiveBuffer].size());
  assertion(sizeOfNewDeployBuffer>=numberOfMessagesSentThisIteration);

  if (numberOfMessagesSentThisIteration < sizeOfNewDeployBuffer) {
    logDebug(
      "switchReceiveAndDeployBuffer(int)",
      "have to copy back " << sizeOfNewDeployBuffer-numberOfMessagesSentThisIteration <<
      " message(s) from the deploy buffer to the receive buffer or rank " << _rank <<
      ", as " << numberOfMessagesSentThisIteration <<
      " message(s) have been sent out while " << sizeOfNewDeployBuffer << " have been received"
    );

    typename std::list<SendReceiveTaskType >::iterator p = _receiveTasks[1-_currentReceiveBuffer].begin();
    std::advance(p, numberOfMessagesSentThisIteration);

    _receiveTasks[_currentReceiveBuffer].insert(
      _receiveTasks[_currentReceiveBuffer].begin(),
      p,
      _receiveTasks[1-_currentReceiveBuffer].end()
    );
    _receiveTasks[1-_currentReceiveBuffer].erase(
      p,
      _receiveTasks[1-_currentReceiveBuffer].end()
    );

    for (
      typename std::list<SendReceiveTaskType >::const_iterator it = _receiveTasks[_currentReceiveBuffer].begin();
      it != _receiveTasks[_currentReceiveBuffer].end();
      it++
    ) {
      logDebug( "switchReceiveAndDeployBuffer(int)", "transferred element (now in receive buffer): " << it->toString() );
    }

    for (
      typename std::list<SendReceiveTaskType >::const_iterator it = _receiveTasks[1-_currentReceiveBuffer].begin();
      it != _receiveTasks[1-_currentReceiveBuffer].end();
      it++
    ) {
      logDebug( "switchReceiveAndDeployBuffer(int)", "remaining deploy element: " << it->toString() );
    }
  }
  else {
    logDebug(
      "switchReceiveAndDeployBuffer(int)",
      "number of received messages from rank " << _rank << " equals number of sent messages: " << numberOfMessagesSentThisIteration
    );
  }

  assertionEquals( static_cast<int>(_receiveTasks[1-_currentReceiveBuffer].size()), numberOfMessagesSentThisIteration);
}


template <class Data, class SendReceiveTaskType, class VectorContainer>
peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::BackgroundThread::BackgroundThread(BoundaryDataExchanger*  boundaryDataExchanger):
  _boundaryDataExchanger(boundaryDataExchanger),
  _state(State::Running) {
}


template <class Data, class SendReceiveTaskType, class VectorContainer>
peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::BackgroundThread::~BackgroundThread() {
}


template <class Data, class SendReceiveTaskType, class VectorContainer>
std::string peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::BackgroundThread::toString(State state) {
  switch (state) {
    case State::Running:
      return "running";
    case State::Terminate:
      return "terminate";
  }

  return "<undef>";
}


template <class Data, class SendReceiveTaskType, class VectorContainer>
bool peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::BackgroundThread::operator()() {
  #if !defined(MPIHeapUsesItsOwnThread) and defined(MultipleThreadsMayTriggerMPICalls) and (SharedMemoryParallelisation)
  assertionMsg( false, "not never enter this operator" );
  #endif

  tarch::multicore::RecursiveLock lock( tarch::services::Service::receiveDanglingMessagesSemaphore );
  bool result = true;

  switch (_state) {
    case State::Running:
      {
        _boundaryDataExchanger->receiveDanglingMessages();
        // A release fence prevents the memory reordering of any read or write which precedes it in program order with any write which follows it in program order.
        //std::atomic_thread_fence(std::memory_order_release);
      }
      break;
    case State::Terminate:
      result = false;
      break;
  }

  lock.free();
  return result;
}


template <class Data, class SendReceiveTaskType, class VectorContainer>
std::string peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::BackgroundThread::toString() const {
  return toString(_state);
}


template <class Data, class SendReceiveTaskType, class VectorContainer>
void peano::heap::BoundaryDataExchanger<Data,SendReceiveTaskType,VectorContainer>::BackgroundThread::terminate() {
  _state = State::Terminate;
}

