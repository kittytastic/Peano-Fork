<h2>Logical topology on MPI ranks</h2>

<a href="_IMAGE_DIRECTORY_/topology.pdf"><img src="_IMAGE_DIRECTORY_/topology.png" /></a>


<h2>Workload</h2>

<h3>Workload per rank</h3>

<a href="_IMAGE_DIRECTORY_/workload-per-rank.pdf"><img src="_IMAGE_DIRECTORY_/workload-per-rank.png" /></a>
<a href="_IMAGE_DIRECTORY_/workload-per-rank.symlog.pdf"><img src="_IMAGE_DIRECTORY_/workload-per-rank.symlog.png" /></a>

<p>
  The filled region is the actual local work volume of a rank. It has to be 
  smaller than the region of responsibility that might overlap the actual domain.
  Rank 0 should deploy all of its work to other ranks and focus solely on load 
  balancing and simulation administration, i.e. its filled region should be empty.
  The vertical lines highlight those ranks with a particular high workload.
  If you want to speed up your code, it might be reasonable to try to reduce the
  load on these ranks or to make them benefit from multiple cores significiantly.
</p>

<h3>Workload per rank as heat map</h3>

<a href="_IMAGE_DIRECTORY_/heatmap-all.pdf"><img src="_IMAGE_DIRECTORY_/heatmap-all.png" /></a>

<p>
  Let work be defined as overlap with the computational domain minus
  the domain overlap of its children.
  Ranks with the largest work are filled white.
  In 3D, the y-axis is populated by 2-tuples indicating the
  y and z offset of a rank's subdomain with respect to the bounding box
  offset, e.g. the rank with x-coordinate 1 and y-z-coordinate (1,1)
  on level 1 is placed in the center of the bounding box.
  The same linearisation is applied on deeper levels of the tree.
</p>

<p>
  The levels are plotted on top of each other. The deepest tree level
  is plotted on top.
</p>

<p>
  Below, the global heat map is decomposed into heat maps per level of the tree.
  (Heat colouring is still performed according to the maximum global workload.)
</p>

_HEAT_MAPS_PER_LEVEL_

<h3>Workload per node</h3>

<a href="_IMAGE_DIRECTORY_/workload-per-node.pdf"><img src="_IMAGE_DIRECTORY_/workload-per-node.png" /></a>
<a href="_IMAGE_DIRECTORY_/workload-per-node.symlog.pdf"><img src="_IMAGE_DIRECTORY_/workload-per-node.symlog.png" /></a>

<p>
 The plots illustrate the aggregated work per compute node. A strong imbalance 
 here on the one hand implies that the work itself is imbalanced - at least as 
 long as we assume roughly the same number of ranks per node. On the other hand,
 it suggests that we quickly might run into out of memory problems.
</p>

<h3>Memory usage per rank</h3>

<a href="_IMAGE_DIRECTORY_/memory-per-rank.pdf"><img src="_IMAGE_DIRECTORY_/memory-per-rank.png" /></a>

<p>
 The plot illustrates the memory usage per rank. If the plot is empty, please ensure that you do not filter 
 out information from runners::Runner::runAsWorker.
</p>

<h2>Domain decomposition</h2>

<h3> Node-wisely </h3>

<a href="_IMAGE_DIRECTORY_/dd.pdf"><img src="_IMAGE_DIRECTORY_/dd.png" /></a>


<p>
 This view is not available for d=3.
</p>

<h3> Level-wisely </h3>

<a href="_IMAGE_DIRECTORY_/dd.level1.pdf"><img src="_IMAGE_DIRECTORY_/dd.level1.png" /></a>
<a href="_IMAGE_DIRECTORY_/dd.level2.pdf"><img src="_IMAGE_DIRECTORY_/dd.level2.png" /></a>
<a href="_IMAGE_DIRECTORY_/dd.level3.pdf"><img src="_IMAGE_DIRECTORY_/dd.level3.png" /></a>
<a href="_IMAGE_DIRECTORY_/dd.level3.pdf"><img src="_IMAGE_DIRECTORY_/dd.level4.png" /></a>
<a href="_IMAGE_DIRECTORY_/dd.level3.pdf"><img src="_IMAGE_DIRECTORY_/dd.level5.png" /></a>
<a href="_IMAGE_DIRECTORY_/dd.level3.pdf"><img src="_IMAGE_DIRECTORY_/dd.level6.png" /></a>


<p>
  The gallery above illustrates the domain decomposition for up to the first six levels of the grid 
  resolution. If finer levels are split up, too, you find further plots in the output directory.
</p>
