//
// Peano4 data file
// Generated by Peano's Python API
// www.peano-framework.org
// This is generated. Be careful with adding your own stuff
//
{% macro fullyQualified(delim) -%}
{{NAMESPACE | join(delim)}}{{ delim }}{{CLASSNAME}}
{%- endmacro -%}

#ifndef {{ fullyQualified("_") }}_H_
#define {{ fullyQualified("_") }}_H_


#include <string>

#ifdef Parallel
  #include <mpi.h>
  #include <functional>
#endif

#include "tarch/la/Vector.h"
#include "tarch/mpi/Rank.h"
#include "tarch/services/ServiceRepository.h"
#include "peano4/utils/Globals.h"

#include "peano4/datamanagement/VertexMarker.h"


{% for item in NAMESPACE -%}
namespace {{ item }} {
{% endfor %}
  struct {{CLASSNAME}};
{% for item in NAMESPACE %}
}
{%- endfor %}


struct {{ fullyQualified("::") }} {
  public:

    {{CLASSNAME}}() {}
    {{CLASSNAME}}(tarch::la::Vector<Dimensions,double>  __debugX, tarch::la::Vector<Dimensions,double>  __debugH, int  __numTasksRemaining);

    tarch::la::Vector<Dimensions,double>   getDebugX() const;
    void   setDebugX(const tarch::la::Vector<Dimensions,double>& value);
    double   getDebugX(int index) const;
    void   setDebugX(int index, double value);
    tarch::la::Vector<Dimensions,double>   getDebugH() const;
    void   setDebugH(const tarch::la::Vector<Dimensions,double>& value);
    double   getDebugH(int index) const;
    void   setDebugH(int index, double value);
    int   getNumTasksRemaining() const;
    void   setNumTasksRemaining(int value);


    #ifdef Parallel
    /**
     * @return The rank of the sender of an object. It only make ssense to call
     *         this routine after you've invoked receive with MPI_ANY_SOURCE.
     */
    int getSenderRank() const;

    /**
     * To be called prior to any MPI usage of this class.
     */
    static void initDatatype();
    static void shutdownDatatype();

    /**
     * In DaStGen (the first version), I had a non-static version of the send
     * as well as the receive. However, this did not work with newer C++11
     * versions, as a member function using this as pointer usually doesn't
     * see the vtable while the init sees the object from outside, i.e.
     * including a vtable. So this routine now is basically an alias for a
     * blocking MPI_Send.
     */
    static void send(const {{ fullyQualified("::") }}& buffer, int destination, int tag, MPI_Comm communicator );
    static void receive({{ fullyQualified("::") }}& buffer, int source, int tag, MPI_Comm communicator );

    /**
     * Alternative to the other send() where I trigger a non-blocking send an
     * then invoke the functor until the corresponding MPI_Test tells me that
     * the message went through. In systems with heavy MPI usage, this can
     * help to avoid deadlocks.
     */
    static void send(const {{ fullyQualified("::") }}& buffer, int destination, int tag, std::function<void()> waitFunctor, MPI_Comm communicator );
    static void receive({{ fullyQualified("::") }}& buffer, int source, int tag, std::function<void()> waitFunctor, MPI_Comm communicator );

    static void sendAndPollDanglingMessages(const {{ fullyQualified("::") }}& message, int destination, int tag );
    static void receiveAndPollDanglingMessages({{ fullyQualified("::") }}& message, int source, int tag );
    #endif

    void merge(const {{ fullyQualified("::") }}& neighbour, const peano4::datamanagement::VertexMarker& marker);

    static bool receiveAndMerge(const peano4::datamanagement::VertexMarker& marker);
    static bool send(const peano4::datamanagement::VertexMarker& marker);
    static bool storePersistently(const peano4::datamanagement::VertexMarker& marker);
    static bool loadPersistently(const peano4::datamanagement::VertexMarker& marker);

    std::string toString() const;

    int addTask();
    int removeTask();

  private:
    tarch::la::Vector<Dimensions,double>   _debugX;
    tarch::la::Vector<Dimensions,double>   _debugH;
    int _numTasksRemaining = 0;


    #ifdef Parallel
    public:
      static MPI_Datatype  Datatype;
    private:
    int                  _senderDestinationRank;
    #endif

};


#endif
