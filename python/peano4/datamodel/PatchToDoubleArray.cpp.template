#include "{CLASSNAME}.h"


// Some MPI routines require the loops
#include "peano4/utils/Loop.h"


 
{FULL_QUALIFIED_CLASSNAME}::{CLASSNAME}() {{
}}


{FULL_QUALIFIED_CLASSNAME}::{CLASSNAME}(const {CLASSNAME}& other) {{
  #if PeanoDebug>=1
  _debugX = other._debugX;
  _debugH = other._debugH;
  #endif

  #if Dimensions==2
  std::copy(other.value, other.value+{CARDINALITY_2D},value);
  #else
  std::copy(other.value, other.value+{CARDINALITY_3D},value);
  #endif
}}


{FULL_QUALIFIED_CLASSNAME}& {FULL_QUALIFIED_CLASSNAME}::operator=(const {CLASSNAME}& other) {{
  #if PeanoDebug>=1
  _debugX = other._debugX;
  _debugH = other._debugH;
  #endif
  
  #if Dimensions==2
  std::copy(other.value, other.value+{CARDINALITY_2D},value);
  #else
  std::copy(other.value, other.value+{CARDINALITY_3D},value);
  #endif
  return *this;
}}


std::string {FULL_QUALIFIED_CLASSNAME}::toString() const {{
  std::string result = std::string("()");
  return result;
}}


#if PeanoDebug>=1

void {FULL_QUALIFIED_CLASSNAME}::setDebugX( const tarch::la::Vector<Dimensions,double>& data ) {{
  _debugX = data;
}}


void {FULL_QUALIFIED_CLASSNAME}::setDebugH( const tarch::la::Vector<Dimensions,double>& data ) {{
  _debugH = data;
}}


tarch::la::Vector<Dimensions,double> {FULL_QUALIFIED_CLASSNAME}::getDebugX() const {{
  return _debugX;
}}


tarch::la::Vector<Dimensions,double> {FULL_QUALIFIED_CLASSNAME}::getDebugH() const {{
  return _debugH;
}}

#endif


{MERGE_METHOD_DEFINITIONS}

  
#ifdef Parallel
void {FULL_QUALIFIED_CLASSNAME}::initDatatype() {{
  {FULL_QUALIFIED_CLASSNAME}  instances[2];
 
  #if PeanoDebug>=1
    MPI_Datatype subtypes[] = {{ MPI_DOUBLE }};

    #if Dimensions==2
    int blocklen[] = {{ Dimensions, Dimensions, {CARDINALITY_2D} }};
    #else
    int blocklen[] = {{ Dimensions, Dimensions, {CARDINALITY_3D} }};
    #endif

    const int NumberOfAttributes = 3;
  #else   
    MPI_Datatype subtypes[] = {{ MPI_DOUBLE }};
    
    #if Dimensions==2
    int blocklen[] = {{ {CARDINALITY_2D} }};
    #else
    int blocklen[] = {{ {CARDINALITY_3D} }};
    #endif
  
    const int NumberOfAttributes = 1;  
  #endif

  MPI_Aint  baseFirstInstance;
  MPI_Aint  baseSecondInstance;
  MPI_Get_address( &instances[0], &baseFirstInstance );
  MPI_Get_address( &instances[1], &baseSecondInstance );
  MPI_Aint  disp[ NumberOfAttributes ];
  int       currentAddress = 0;
  #if PeanoDebug>=1
  MPI_Get_address( &(instances[0]._debugX.data()[0]), &disp[currentAddress] );
  currentAddress++;
  MPI_Get_address( &(instances[0]._debugH.data()[0]), &disp[currentAddress] );
  currentAddress++;
  #endif
  MPI_Get_address( &(instances[0].value), &disp[currentAddress] );
  currentAddress++;

  MPI_Aint offset = disp[0] - baseFirstInstance;
  MPI_Aint extent = baseSecondInstance - baseFirstInstance - offset;
  for (int i=NumberOfAttributes-1; i>=0; i--) {{
    disp[i] = disp[i] - disp[0];
  }}

  int errorCode = 0; 
  MPI_Datatype tmpType; 
  errorCode += MPI_Type_create_struct( NumberOfAttributes, blocklen, disp, subtypes, &tmpType );
  errorCode += MPI_Type_create_resized( tmpType, offset, extent, &Datatype );
  errorCode += MPI_Type_commit( &Datatype );
  errorCode += MPI_Type_free( &tmpType );
  if (errorCode) std::cerr << "error constructing MPI datatype in " << __FILE__ << ":" << __LINE__ << std::endl;
}}


void {FULL_QUALIFIED_CLASSNAME}::shutdownDatatype() {{
  MPI_Type_free( &Datatype );
}}
  

MPI_Datatype   {FULL_QUALIFIED_CLASSNAME}::Datatype;
#endif
