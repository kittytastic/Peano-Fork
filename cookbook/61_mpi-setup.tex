\section{Reducing the MPI grid setup and initial load balancing overhead}


\chapterDescription
  {
    Around 30 minutes.
  }
  {
    A working MPI code.
  }


In this section, we assume that you've a reasonable load balancing and that you
were able to postprocess your performance analysis outputs. We discuss issues
that arise over and over again for parallel applications.


\vspace{0.6cm}
{\bf \large Massive grid on rank 0 with long redistribution phase
afterwards}

\begin{center}
  \includegraphics[width=0.5\textwidth]{61_mpi-setup/performance-analysis-output.pdf}
\end{center}


\begin{smell}
There are ranks (notably rank 0) that are assigned lots of cells and vertices
and then this number decreases though the grid should be more or less static.
\end{smell}

\noindent
We observe this behaviour either through a performance analysis (see plot above)
or by outputs from the state where the grid depth immediately goes up to the
maximum depth while the load balancing still splits up things. 
The iterations already are very expensive; obviously as the grid is already in
place but the ranks are not all employed.


The reason for this behaviour can be found in the semantics of
\texttt{createVertex} and \linebreak
\texttt{touchVertexFirstTime}.
Both operations try to refine the grid around the respective vertex immediately. 
Only if circumstances such as a parallel partitioning running through this
vertex---the refinement instruction then first has be distributed to all ranks
holding a copy of this vertex---do not allow Peano to realise the refinement
immediately, the refinement is postponed to the next iteration.
In many parallel codes, all the refinement calls pass through immediately on
rank 0 before it can spawn any rank.
This leads to the situation that the whole grid is in one sweep built up on the
global master and afterwards successively distributed among the ranks.


Such a behaviour is problematic: the global rank might run out of memory, lots
of data is transferred, and the sweeps over the whole grid on rank 0 are
typically pretty expensive. 
A distributed grid setup is advantageous.

\begin{solution}
Switch from an aggressive
refinement into an iterative grid refinement strategy to allow the ranks to
deploy work throughout the grid construction and thus build up the grid in parallel and avoid the transfer of whole grid
blocks due to rebalancing.
\end{solution}

\noindent
The simplest materialisation of this idea is to 
move your \texttt{refine()} calls from the creational or touch first
events into \texttt{touchVertexLastTime()}:
As a consequence, setting up a (rather regular) grid of depth $k$ requires at
least $k$ iterations.

\begin{remark}
 I typically extract the intial grid construction decision into a function of 
 its own. If \texttt{-DParallel} is used, I invoke grid constructions only 
 from \texttt{touchVertexLastTime}. Otherwise, I use it in the creational 
 routines for inner and boundary vertices. For this strategy, we have to ensure
 that touch last is not set to \texttt{Nop} in the specification attributes if
 we compile with MPI.
\end{remark}

In a second step, you might consider to extend your grid only every second
traversal.
Everytime you rebalance your grid, Peano disables dynamic load balancing
for a couple of iterations (three or four). Throughout these iterations, it
can recover all adjacency information if the grid itself changes as well.
Consequently, it does make sense to add a couple of adapter runs after each
grid modification that to not change the grid structure: When you know that
you have an adapter that changes the grid, apply afterwards an adapter that
does not change the grid for a couple of times. This way, you ensure that no
mpi rank runs out of memory. The grid generation does not overtake the rebalancing.
I often do not use an additional adapter but ensure that refines are only called
if the tree traversal direction is not inverted: 
Peano runs forth and back through the grid, so this effectively switches off
refinement every second iteration. 

The deluxe version is a code that refines only every fifth traversal (which is
one count less than what we need to make a grid balanced). 
I often introduce a global counter in my grid creation mapping, increment this
counter in \textt{beginIteration} and also use it to veto refinements:
\begin{code}
void exahype::mappings::RegularMesh::beginIteration(
  exahype::State& solverState
) {
  ADERDGCellDescriptionHeap::getInstance().setName("ADERDGCellDescriptionHeap");
  DataHeap::getInstance().setName("DataHeap");
  _vetoRefinement = _traversalCounter%5!=0 
                 && tarch::parallel::Node::getInstance().getNumberOfNodes()>1;
  _traversalCounter++;
}
\end{code} 
\noindent
Obviously, this is a very bad idea if no idle nodes are available anymore. 
In this case, it is better not to veto the refinement anymore.
This situation is discussed in the following bad smell. 



\vspace{0.6cm}
{\bf \large Incremental grid setup though detailed grid structure is
known/all nodes are already busy}


\begin{smell}
The grid is static, but, nevertheless, Peano needs lots of iterations to finally
build it up. Even worse, there are no idle nodes left and we know analytically
what the grid should look like.
The grid construction thus should rush through in one iteration.
\end{smell}


@todo Introduce new flag in State, dass in einen Urgent Refine Modus verfaellt
@todo Das wird auf Rank 0 gesetzt in beginIteration() und propagiert sich
dadurch automatisch durch
@todo Jetzt kicken wir das modulo 5 wieder raus
@todo Und weiterhin machen wir im regulaeren Teil (Struktur a priori bekannt)
ein enforceRefine



{\bf The load balancing kicks in immediately while I build up my grid but it
yields non-reasonable partitions}


\begin{smell}
t.b.d.
\end{smell}


If there are idle nodes, we may not use them.

If you identify ranks whose local load decreases incrementally, these are ranks
that step by step fork more of their work to other ranks. 
In principle, this is fine and a result of load balancing. 
For reasonably static setups, it however is irritating: 
why is there a transition phase where obviously solely data is
redistributed?





To find out when a grid has been constructed and balanced completely, the
repository provides an operation. Instead of writing something along the lines

\begin{code}
  repository.switchToSetup();
  repository.iterate();
\end{code}

\noindent
you have to write
\begin{code}
  repository.switchToSetupExperiment();
  do {
    repository.iterate();
  } while ( !repository.getState().isGridBalanced() );
\end{code}




  \item Once all ranks have obtained `their' partition, it does not make sense
  to continue to build up at most one grid level per sweep. In this case, you
  have to reliase an inverse pattern compared to the pattern sketched in the
  first bullet point. Such a situation is easy to spot: it typically
  materialises in a slow increase of total/global vertices while the fork
  statistics show that no forks happen anymore. Compare the two plots below:
  \begin{center}
    \includegraphics[width=0.4\textwidth]{61_mpi-setup/grid-construction.pdf}
    \includegraphics[width=0.4\textwidth]{61_mpi-setup/fork-behaviour.pdf}
    \\
    {
    \footnotesize
    }
  The grid construction requires about 80s while the last forks are tracked
  at t=0.3s. Starting from t=0.3s, one could build up the grid one sweep.
  \end{center}
  \begin{remark}
    Peano parallel code offers an operation \texttt{enforceRefine()} on
    the vertices that you can use to tackle this problem. Use with care and 
    read through the documentation in code.
  \end{remark}


\paragraph{Related pitfalls \& ideas}

As always, the devil is in the details:
\begin{itemize}
  \item  For many load balancing algorithms, it does make sense to create an
  initial grid of depth $\hat k <k$ on your rank 0 before you do any load
  balancing. This allows the load balancing metric to get a first idea about
  what the grid will look like and then to switch on load
  balancing. This can be done with
  \begin{code}
  peano::parallel::loadbalancing::Oracle::getInstance().activateLoadBalancing(false);
  // set up grid up to a certain level
  peano::parallel::loadbalancing::Oracle::getInstance().activateLoadBalancing(true);
  \end{code}









  \item If you are using the heap data structure, it furthermore makes sense to split up
the initialisation into a grid setup and a data struture initialisation.
You balance and distribute the grid setup following the recommendations above
and then in one additional sweep initialise the heap.
You initialise the heap as late as possible and thus avoid unneccesary
administrative overhead.
\end{itemize}


\paragraph{Pattern for static grid setup}

Most codes at least start form a static grid partitioning and globally know
what the initial grid looks like.
It then has proven of value to do the following:

\begin{enumerate}
  \item Determine a certain grid level that should be used to do an initial load
  balancing. If you have a regular grid, this might be the coarsest grid level
  that could be deployed among all involved ranks:
  \begin{code}
_coarsestRegularLevelUsedForDD = 0;
int ranksUsedSoFar             = 0;
int increment                  = 1;
while (ranksUsedSoFar < tarch::parallel::Node::getInstance().getNumberOfNodes()) {
  ranksUsedSoFar += increment;
  increment      *= THREE_POWER_D;
  _coarsestRegularLevelUsedForDD ++;
}
  \end{code}
  Typically, I determine this level in \texttt{beginIteration()} of the mapping
  that constructs the initial grid. It is thus determined in parallel on all
  ranks as soon as a rank joins the game.
  \item I make the grid setup refine the grid in \texttt{touchVertexLastTime},
  i.e.~the grid is created with one level per sweep. As this part of the code
  runs in parallel, we run over the grid $k'$ times, add one level per sweep (so
  $k'$ becomes the depth of the tree), and at the same time distribute the grid
  among the ranks. We successively flood the MPI nodes. However, we continue to
  add new levels if and only if we do not exceed the initial grid depth
  determined in step 1:
  \begin{code}
...::touchVertexLastTime(...) {
  if (
    shallRefine(fineGridVertex,fineGridH)
    &&
    coarseGridVerticesEnumerator.getLevel() < _coarsestRegularLevelUsedForDD
  ) {
    fineGridVertex.refine();
  }
  ...
}  
  \end{code}
  \item I make all the ranks switch off dynamic load balancing the first time
  the global master runs a step on all ranks out there:
  \begin{code}
void picard::runners::Runner::runGlobalStep() {
  // assertion( !peano::parallel::loadbalancing::Oracle::getInstance().
  // isLoadBalancingActivated() );

  peano::parallel::loadbalancing::Oracle::getInstance().activateLoadBalancing(false);
}
  \end{code}
  For this, I remove the assertion original put in by PDT. I know what I'm
  doing, as \ldots
  \item I run through the grid until it becomes stationary, i.e.~does not change
  anymore and is properly distributed. Due to the variable
  \texttt{\_coarsestRegularLevelUsedForDD} this will require a couple of sweeps
  but will not set up the whole grid. Next, I switch off load balancing
  globally. Finally, I rerun the grid construction twice. The runner then
  resembles
  \begin{code}
repository.switchToCreateGrid();
do {
  repository.iterate();
} while ( !repository.getState().isGridBalanced() );

repository.runGlobalStep();
runGlobalStep();

repository.iterate();
repository.iterate();
  \end{code}
  \item So far, the last two iterates do not change the grid anymore and they
  notably do not build up the whole grid if the grid is truncated by
  \texttt{\_coarsestRegularLevelUsedForDD}. I finally return to the mapping's
  touch vertex last time event and continue to refine if the load balancing is
  switched off. This refinement will kick in the in first of the two additional
  \texttt{iterate} commands.
  \begin{code}
...::touchLastTime(...) {
  if (
    shallRefine(fineGridVertex,fineGridH)
    &&
    !peano::parallel::loadbalancing::Oracle::getInstance().isLoadBalancingActivated()
  ) {
    fineGridVertex.refine();
  }
}
  \end{code}
  \item This new fragment will make the last \texttt{iterate} introduce one
  additional level that is finer than \texttt{\_coarsestRegularLevelUsedForDD}. 
  When it invokes the corresponding creational routines, we now use
  \texttt{enforceRefine} to build up the remaining grid parts in one sweep. 
  \begin{code}
...::createInnerVertex(...) {
  if ( 
    shallRefine(fineGridVertex,fineGridH) 
    &&
    !peano::parallel::loadbalancing::Oracle::getInstance().isLoadBalancingActivated()
  ) {
    fineGridVertex.enforceRefine();
  }

}
\end{code}
  Exactly the same has to be done within \texttt{createBoundaryVertex}.
\end{enumerate}



