\chapter{Troubleshooting}


\section{Python API}

\begin{itemize}
  \item \textbf{\texttt{import peano4} yields not found error}. You haven't set
  your \texttt{PYTHONPATH} properly. Ensure that it points to \Peano's
  \texttt{python} subdirectory. So use something similar to 
  \begin{code}
export PYTHONPATH=~/git/Peano/python
  \end{code}
  \item \textbf{/usr/bin/ld: cannot find -lExaHyPE2Core3d\_trace}. Your Python
  script has not been able to locate the \Peano\ libraries. You can either tweak
  your systems parameters, i.e.~set the search pathes, or you can manually add
  the library:
\begin{code}
peano4_project.output.makefile.add_library(
  project.getLibrary(peano4.output.CompileMode.Debug), 
  "mypath/src/exahype2" 
)
\end{code}
%   \item \textbf{No unit tests seem to run.} Peano4 builds a set of libraries that are linked into 
%     the actual application. It seems that some linkers consider the unit tests to be dead code,
%     as the actual test routines indeed are never called (by the library). See the documentation in 
%     \texttt{tarch/tests/TestCase.h} for some details. 
  \item To be continued \dots
\end{itemize}


\section{Compilation}



% /usr/bin/ld: ../../../../src/exahype2/libExaHyPE2Core2d_debug.a(libExaHyPE2Core2d_debug_a-PatchUtils.o): relocation R_X86_64_32 against symbol `_ZSt4cerr@@GLIBCXX_3.4' can not be used when making a PIE object; recompile with -fPIE


\begin{itemize}
  \item \textbf{ My compiler terminates with `error: unknown
   attribute 'optimize' ignored*`}. We have seen this issue notably on macOS
   with CLANG replacing GCC. Unfortunately, CLANG seems to pretend to be GNU on
   some systems and then the wrong header is included. Ensure that
   \texttt{CompilerSpecificSettings.h}\footnote{Peano relies on a header \texttt{tarch/compiler/CompilerSpecificSettings.h}.
Whenever I find incompatibilities between different compilers, I try to resolve them through this file. 
The header reads out some compiler preprocessor directives and then includes the
one it find most appropriate. 
You may always include your own file derived from one of the other headers in
the directory.} includes a file such that the
   compiler
   \linebreak
   \texttt{CompilerCLANG} is defined.
  \item \textbf{ My compiler requires ages to translate the unit tests}. Ensure
  that the flag \linebreak \texttt{UseTestSpecificCompilerSettings} is enabled.
  This effectively disables any optimisation for the unit tests. Without the
  flag, unit test can become really big due to template instantiation.
  \item \textbf{\texttt{./configure} crashes when I try to translate with TBB}.
  I have seen this on a couple of systems and found it annoying. 
  First, ensure that configure
  find the TBB header. To ensure it does, check that \texttt{CXXFLAGS} are properly
  set. Next, set \texttt{LDFLAGS=-Lmypath}. Usually, I add \texttt{-ltbb} to 
  this parameter and then everything is fine. However, some configures then don't 
  like the configure script anymore. So I instead use \texttt{export PEANO\_LDFLAGS="-ltbb"}. 
  For some systems, it then still remains necessary to add pthreads, too.
  \item \textbf{The linker says that the vtk libraries were not found}.
  VTK can be tricky to get up and running. See a comprehensive discussion on VTK
  issues in Chapter \ref{chapter:vtk}. It includes compiler/linker issues.
  \item To be continued \dots
\end{itemize}



\section{Linking}

\begin{itemize}
  \item \textbf{If I use Fortran as well, I get errors alike \emph{undefined
  reference to 'for\_write\_seq\_list' or 'for\_stop\_core'}}. This tells you
  that the linker failed to find the Fortran standard libraries. For the GNU
  cmopilers, you need for example the flag \texttt{-lgfortran}, while the Intel
  compiler requires you to add \ldots
  To add these flags, set the \texttt{LDFLAGS} before you run
  \texttt{configure}:
  \begin{code}
# GNU
export LDFLAGS=-lgfortran

# Intel
export LDFLAGS=-lifcore
  \end{code}
  On the Python API side, I considered adding these flags to the template
  makefile or offering an API function to add them manually to the makefile from
  the Python script. I dropped these ideas, as these libraries are specific to
  the build and hardware system and have nothing to do with a particular
  application.
%   \item \textbf{No unit tests seem to run.} Peano4 builds a set of libraries that are linked into 
%     the actual application. It seems that some linkers consider the unit tests to be dead code,
%     as the actual test routines indeed are never called (by the library). See the documentation in 
%     \texttt{tarch/tests/TestCase.h} for some details. 
  \item To be continued \dots
\end{itemize}




\section{Debugging}
\begin{itemize}
  \item \textbf{I can't see my log/trace statements.} There's a whole set of
    things that can go wrong.
    \begin{itemize}
      \item Check that your log filter specifies the {\bf class name}. In Peano
      3, there was the opportunity to filter messages out on a routine level.
      For efficiency reasons, I had to remove this. So now you can only filter
      per class. Ensure you don't specify methods. If you do so, Peano 4 thinks
      your method name is a (sub)class name and won't apply your filter.
      \item Ensure that your main code is translated with
      a reasonable debug flag. The makefile has to translate your code with
      \texttt{-DPeanoDebug=4} (or 1 if you are only interested in tracing).
      \item     Ensure you link to libraries which are built with a reasonable debug level. 
    For this, run your code and search for the header
    \begin{code}
build: 2d, no mpi, C++ threading, debug level=4
    \end{code}
    on the terminal. Again, has to be 1 or 4 at least.
    \item Ensure you have a
    whitelist entry for the routine you are interested in. Ensure that it is the
    right level (\texttt{TargetDebug} or \texttt{TargetTrace}, respectively):
    \begin{code}
tarch::logging::LogFilter::getInstance().addFilterListEntry( 
  tarch::logging::LogFilter::FilterListEntry(
    tarch::logging::LogFilter::FilterListEntry::TargetInfo,
    tarch::logging::LogFilter::FilterListEntry::AnyRank,
    "examples::algebraicmg",
    tarch::logging::LogFilter::FilterListEntry::WhiteListEntry
  )
);
    \end{code}
    \item If you are still unsure which log filters are active, insert
    \begin{code}
tarch::logging::LogFilter::getInstance().printFilterListToCout();
    \end{code}
    into your code and see which entries it does enlist.
  \end{itemize}
  \item \textbf{gdb says ``not in executable format: file format not
    recognized''}. automake puts the actual executable into a \texttt{.libs}
    directory and creates bash scripts invoking those guys. Change into
    \texttt{.libs} and run gdb directly on the executable. Before you do so,
    ensure that \texttt{LD\_LIBRARY\_Path} points to the directory containing
    the libraries. Again, those guys are stored in a \texttt{.libs}
    subdirectory, so the library path should point to that subdirectory. 
%   Most of these tweaks
%   should not be necessary if you install Peano properly through \texttt{make
%   install}.
  \item To be continued \dots
\end{itemize}
 
 
 
 



\section{MPI troubleshooting}
\begin{itemize}
  \item \textbf{My code crashes unexpectedly with malloc/free errors}. Please
  rerun your code with Intel MPI and the flag \texttt{-check\_mpi}. You should
  not get any error reports. If you do, we have had serious problems as some of
  Peano's classes use \texttt{MPI\_C\_BOOL} or \texttt{MPI\_CXX\_BOOL}. They
  seem not be supported properly.
  \item To be continued \dots
\end{itemize}



% 
% \begin{itemize}
%   \item \textbf{ I get tons of warning alike ``warning \#603: ``typeid'' is reserved for future use as a keyword'' }.
%   Peano's pdt adds a compile flag \texttt{-fno-rtti} to the makefile as the core framework does 
%   not need type information at runtime and codes thus become smaller and faster. TBB however 
%   seems to use runtime information, so you have to remove this flag from your makefile if you 
%   want to get rid of all the warning. 
% \end{itemize}
% 
% \section{MPI troubleshooting}
% 
% \begin{itemize}
%   \item \textbf{ Does Peano support MPI-2?}. Yes, you can switch to MPI-2 if you
%   add \texttt{-DMPI2}. I have experienced some issues with MPI-2 implementations and
%   user-defined datatypes and thus decided to make MPI 1.3 the default. If you
%   switch to MPI-2 and experience problems, you  mgiht want to have a look into
%   any \texttt{records} directory and search for \texttt{initDatatype} to
%   understand where issues arise.
%   \item \textbf{ I've altered my data types and MPI starts to crash}. There are
%   multiple reasons why user-defined data types start to fail. Here are some
%   ideas to follow-up:
%     \begin{enumerate}
%       \item Compile with \texttt{-DAsserts}. We augment all parts of Peano with
%       lots and lots of assertions, so they might help you to hunt down bugs.
%       \item We have seen some compilers fail if you label some attributes of
%       (vertex) data types with \texttt{expose}. Expose does alter the
%       visibility, and we came to believe that these visibility modifications
%       make some compilers reorder class attributes which in turns means that the
%       MPI data types hold invalid byte offsets.
%     \end{enumerate}
%   \item \textbf{ My MPI version complains about a datatype
%   \texttt{MPI\_CXX\_BOOL}}.
%     We have seen this with some older MPI versions. A quick fix is to translate
%     your code with the additional argument
%     \texttt{-DMPI\_CXX\_BOOL=MPI\_C\_BOOL}.
%   \item \textbf{ My code deadlocks once I increase the rank count. It seems that
%     the ranks cannot even register at the global node pool anymore.}
%     I've seen this bug with Intel MPI and Omnipath recently. It seems that some
%     MPI implementations struggle to handle many polling non-blocking operations:
%     If multiple ranks are deployed on one node, $k$ of them register at the node
%     pool and then eagerly poll the global master for work. As a result, the
%     $k+1$th rank cannot send out its registration message. The global master in
%     return does not start the actual computation before all ranks have
%     registered if you use the corresponding wait call. There are multiple
%     solutions/things you can try:
%     \begin{enumerate}
%       \item Remove the \texttt{waitForAllNodesToBecomeIdle} calls from your
%       code. Your code might not need it anyway.
%       \item Deploy only one MPI rank per node/interconnect.
%       \item Change into the directory \texttt{tarch/compiler} and find the right
%       compiler-specific header for your system. Change Peano's load balancing
%       data exchange into a blocking MPI:
%       \begin{code}
%       #define SendAndReceiveLoadBalancingMessagesBlocking    -1
%       \end{code}
%     \end{enumerate}
%     However, the best solution is to consult your supercomputer's documentation
%     and to configure the fabric accordingly. On Durham's supercomputer, e.g., an
%     additional
%     \begin{code}
%     export I_MPI_FABRICS="tmi"
%     \end{code} 
%     in the SLURM script fixes the issue.
%   \item \textbf{ Once I switch to (Intel) MPI, my code seems to receive invalid
%     data}. We have seen this problem with Intel's MPI (but not with OpenMPI, 
%     e.g.) and it seems that this one makes very strong assumptions about the
%     alignment of data within arrays to optimise the MPI message exchange. I
%     recommend that you write some ping pong tests (one rank sending stuff to
%     another and then this one sending stuff back where the data is compared to
%     the original) and that you ping pong both single vertices and arrays of
%     vertices. This typically uncovers data inconsistencies. One next step then
%     is to translate your code with \texttt{-DnoPackedRecords}. While switching
%     the flag on reduces the memory footprint dramatically (if you don't use
%     lots of double arrays that is), its underlying skipping of the compiler's
%     padding seems to cause problems for Intel MPI. If you find 
%     \texttt{-DnoPackedRecords} working, roll back, i.e.~use 
%     \texttt{-DPackedRecords} (which is the default), and follow Section
%     \ref{section:advanced-mpi:tailor-data-exchange-format}.
%   \item To be continued \ldots
% \end{itemize}



\section{External tools}
\begin{itemize}
  \item \textbf{The Intel tools yield invalid or messed up results}. Ensure 
    you have built your code with \texttt{-DTBB\_USE\_ASSERT
    -DTBB\_USE\_THREADING\_TOOLS}. Please consult the supercomputer/tool remarks
    on page \pageref{section:supercomputers:IntelTools}.
  \item To be continued \dots
\end{itemize}



\section{PDT}

This section is deprecated and is of relevance if and only if you use the Python API with 
the legacy DaStGen support to model application-specific data.


\begin{itemize}
  \item \textbf{ The PDT does not pass as the jar file is built with the wrong
  Java version}. Download the whole Peano project (from sourceforge via subversion),
  change into the directory \texttt{pdt/src}, and run
  \begin{code}
  make clean
  make createParser
  make compile
  make dist
  \end{code}
  Use the \texttt{pdt.jar} from the \texttt{pdt} directory now. 
\end{itemize}


%update-alterantives --config java




