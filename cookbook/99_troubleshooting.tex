\chapter{Troubleshooting}

\section{PDT}

\begin{itemize}
  \item \textbf{ The PDT does not pass as the jar file is built with the wrong
  Java version}. Download the whole Peano project (from sourceforge via subversion),
  change into the directory \texttt{pdt/src}, and run
  \begin{code}
  make clean
  make createParser
  make compile
  make dist
  \end{code}
  Use the \texttt{pdt.jar} from the \texttt{pdt} directory now. 
\end{itemize}


%update-alterantives --config java



\section{Compilation}

Peano relies on a header \texttt{tarch/compiler/CompilerSpecificSettings.h}
whenever I find incompatibilities between different compilers. 
The header reads out some compiler preprocessor directives and then includes the
one it find most appropriate. 
You may always include your own file derived from one of the other headers in
the directory.


\begin{itemize}
  \item \textbf{ My compiler terminates with `error: unknown
   attribute 'optimize' ignored*`}. We have seen this issue notably on macOS
   with CLANG replacing GCC. Unfortunately, CLANG seems to pretend to be GNU on
   some systems and then the wrong header is included. Ensure that
   \texttt{CompilerSpecificSettings.h} includes a file such that the compiler
   \texttt{CompilerCLANG} is defined.
\end{itemize}




\section{Programming}

\begin{itemize}
  \item \textbf{ How can I find out the location of a cell}. See the
  documentation of the \texttt{VertexEnumerator}. The object has routines to query cell
  position, size and level in the spacetree.
  \item \textbf{ The adaptivity pattern lacks behing my adaptivity
  instructions}.
  For most refinement and erase calls, Peano needs at least one iteration to
  reliase them. It tries to do it faster, but usually needs these two
  iterations. Whenever Peano encounters regular subtrees, it tries to store 
  them separately in some arrays and process it way faster than with a classic
  tree. When a refinement command affects such a regular subregion, we first 
  have to find this region (at least one sweep), then break it up, in the
  follow-up iteration remove all veto markers that stopped the regular grid to
  be refined and then we can realise the refinement. If you have an extremely
  rapidely changing grid and you can't wait for Peano to keep pace, you should
  compile with \texttt{-DnoPersistentRegularSubtrees}. This will make the grid
  react quicker to your refinement requests but might slow down the code
  significantly.
  \item \textbf{ When does a grid coarsen?} If you invoke \texttt{erase}, Peano
  bookkeeps this coarsening requests but continues to traverse the grid. The
  actual grid then is actually coarsened in the subsequent grid sweep. If you
  trigger an erase, grid entities will not be destroyed prior to the next
  traversal. In some cases, it even can happen that an erase is neglected
  completely or postponed further:
  \begin{itemize}
    \item If your erase grid parts and refine adjacent grid parts, it can happen
    that the refine overwrites your erase (or the other way round). It depends
    which instruction can be accomodated by the grid first, i.e.~while Peano
    ensures that the grid remains valid and all events are always triggered in
    the right order, the actual grid pattern is non-deterministic from an
    application's point of view if you refine and coarsen at the same time in
    close domain regions.
    \item If you erase a grid region that is completely handled by one rank,
    then this makes the rank unemployed. If this happens, Peano first has to
    release the rank before the erase passes. In this case, you have to wait two
    or three iterations before your erase passes through. While the erase is
    bookkeeped but not realised yet, the state's \texttt{isGridStationary}
    attribute does not hold.
  \end{itemize}
  \item \textbf{ I need the state object in my mapping}. Create a copy of the
  state object in \texttt{beginIteration} and to merge this copy back in \texttt{endIteration}. 
  Peano updates the State itself and the state position in memory is not fixed.
  Do never ever hold a pointer to the state object handed into
  \texttt{beginIteration} or \texttt{endIteration}.
\end{itemize}


\section{Application tailoring}

\begin{itemize}
  \item \textbf{ The code requires too much memory}.
  You can try to compile your code with
  \linebreak \texttt{-DnoPersistentRegularSubtrees}.
  However, this may lead to a severe performance \linebreak
  degradation---notably if you run your code with shared memory parallelisation.
\end{itemize}

\section{MPI troubleshooting}

\begin{itemize}
  \item \textbf{ Does Peano support MPI-2?}. Yes, you can switch to MPI-2 if you
  add \texttt{-DMPI2}. I have experienced some issues with MPI-2 implementations and
  user-defined datatypes and thus decided to make MPI 1.3 the default. If you
  switch to MPI-2 and experience problems, you  mgiht want to have a look into
  any \texttt{records} directory and search for \texttt{initDatatype} to
  understand where issues arise.
  \item \textbf{ I've altered my data types and MPI starts to crash}. There are
  multiple reasons why user-defined data types start to fail. Here are some
  ideas to follow-up:
    \begin{enumerate}
      \item Compile with \texttt{-DAsserts}. We augment all parts of Peano with
      lots and lots of assertions, so they might help you to hunt down bugs.
      \item We have seen some compilers fail if you label some attributes of
      (vertex) data types with \texttt{expose}. Expose does alter the
      visibility, and we came to believe that these visibility modifications
      make some compilers reorder class attributes which in turns means that the
      MPI data types hold invalid byte offsets.
    \end{enumerate}
  \item \textbf{ My MPI version complains about a datatype
  \texttt{MPI\_CXX\_BOOL}}.
    We have seen this with some older MPI versions. A quick fix is to translate
    your code with the additional argument
    \texttt{-DMPI\_CXX\_BOOL=MPI\_C\_BOOL}.
  \item \textbf{ My code deadlocks once I increase the rank count. It seems that
    the ranks cannot even register at the global node pool anymore.}
    I've seen this bug with Intel MPI and Omnipath recently. It seems that some
    MPI implementations struggle to handle many polling non-blocking operations:
    If multiple ranks are deployed on one node, $k$ of them register at the node
    pool and then eagerly poll the global master for work. As a result, the
    $k+1$th rank cannot send out its registration message. The global master in
    return does not start the actual computation before all ranks have
    registered if you use the corresponding wait call. There are multiple
    solutions/things you can try:
    \begin{enumerate}
      \item Remove the \texttt{waitForAllNodesToBecomeIdle} calls from your
      code. Your code might not need it anyway.
      \item Deploy only one MPI rank per node/interconnect.
      \item Change into the directory \texttt{tarch/compiler} and find the right
      compiler-specific header for your system. Change Peano's load balancing
      data exchange into a blocking MPI:
      \begin{code}
      #define SendAndReceiveLoadBalancingMessagesBlocking    -1
      \end{code}
    \end{enumerate}
    However, the best solution is to consult your supercomputer's documentation
    and to configure the fabric accordingly. On Durham's supercomputer, e.g., an
    additional
    \begin{code}
    export I_MPI_FABRICS="tmi"
    \end{code} 
    in the SLURM script fixes the issue.
  \item \textbf{ Once I switch to (Intel) MPI, my code seems to receive invalid
    data}. We have seen this problem with Intel's MPI (but not with OpenMPI,
    e.g.) and it seems that this one makes very strong assumptions about the
    alignment of data within arrays to optimise the MPI message exchange. I
    recommend that you write some ping pong tests (one rank sending stuff to
    another and then this one sending stuff back where the data is compared to
    the original) and that you ping pong both single vertices and arrays of
    vertices. This typically uncovers data inconsistencies. One next step then
    is to translate your code with \texttt{-DnoPackedRecords}. While switching
    the flag on reduces the memory footprint dramatically (if you don't use
    lots of double arrays that is), its underlying skipping of the compiler's
    padding seems to cause problems for Intel MPI. If you find 
    \texttt{-DnoPackedRecords} working, roll back, i.e.~use 
    \texttt{-DPackedRecords} (which is the default), and follow Section
    \ref{section:advanced-mpi:tailor-data-exchange-format}.
  \item To be continued \ldots
\end{itemize}


