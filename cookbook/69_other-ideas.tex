\section{Compiler-based optimisation}

\chapterDescription
  {
    5 minutes
  }
  {
  }


Peano's implementation can be tailed by tons of different compile options. 
At the end of the day, one might have to tailor them on a per-project base. 
As the settings first of all are tied to a particular machine (besides an
application), Peano collects all magic variables within a file called
{\em CompilerSpecificSettings.h}.
For some default platforms, we provide meaningful results.
All of these settings however can be overwritten no the command line.


\subsection*{Single core}

\begin{center}
 \begin{tabular}{p{2cm}p{2cm}p{9.5cm}}
  Compile flag (default) & Alternative & Description \\
  \hline
  {\footnotesize Use\-Manual\-Inlining} 
  & 
  {\footnotesize noUse\-Manual\-Inlining} 
  &
  Compilers are known to implement good heuristics which routines to inline and
  which not to inline. We however found that some compiler struggle with some
  parts of Peano. For these parts, we have added compiler-specific annotations
  that force the compiler to inline. Obviously, the identification of critical
  code parts has been done for few benchmarks only and it might happen that they 
  are invalid or even contra-productive for some applications. 
  \\
  \hline
 \end{tabular}
\end{center}
  

\subsection*{Shared memory}

\begin{center}
 \begin{tabular}{p{2cm}p{2cm}p{8cm}l}
  Compile flag (default) & Alternative & Description & Target \\
  \hline
  {\footnotesize UseTBBs\-Parallel\-For\-And\-Reduce} 
  & 
  {\footnotesize noUse\-TBBs\-Parallel\-For\-And\-Reduce} 
  &
  Different to TBB, Peano's internal task/job system literally is based on
  tasks only. That is, also parallel loops are mapped onto job spawning. For
  TBB-based codes, we found that such a realisation is sometimes disadvantageous
  for larger core counts. The flag allows us to toggle the implementation. 
  & TBB
  \\
  \hline
 \end{tabular}
\end{center}
  
  
  
\subsection*{Distributed memory}

\begin{center}
 \begin{tabular}{p{2cm}p{2cm}p{9.5cm}l}
  Compile flag (default) & Alternative & Description \\
  \hline
  {\footnotesize MPI\-Progression\-Relies\-On\-MPI\-Test} 
  & 
  {\footnotesize noMPI\-Progression\-Relies\-On\-MPI\-Test} 
  &
  Some MPI implementations struggle to transfer data in the background of the
  simulation run. In this case, one has to call \texttt{MPI\_Test} over an over
  again. Each test call allows MPI to progress some messages. Obviously,
  these calls do not come for free and impose some overhead, too. The flag
  allows users to disable/enable this test polling. Unless not pinned
  explicitly to one thread (see below), Peano's hybrid implementation
  distributes the tests to the cores, i.e.~hardware threads call 
  \texttt{MPI\_Test} whenever they become idle.
  \\
  \hline
 \end{tabular}
\end{center}


\noindent
The following variables allow users to tailor the treatment of (logically)
blocking sends and receives.
Their default value always is 0. 
0 means that each blocking operation is mapped onto a non-blocking operation
followed by a while loop over \texttt{MPI\_Test}. 
This seems to be inefficient, but actually these tests are intermixed with calls
to routines of the type \texttt{receiveDanglingMessages} which receive MPI
messages which should be transferred in the background and might congest the MPI
buffers at the moment.
If you set the variables to -1, then Peano replaces this non-blocking
``pattern'' with real blocking sends and receives.
If you set them to something greater than 0, then each non-successful test from
the pattern above is follows by a \texttt{usleep} of the given value. 
This pattern pays off, if too many ranks poll the MPI subsystem or you use
hyperthreading and some code parts that want to do MPI calls too suffer
from starvation.

\begin{itemize}
  \item SendWorkerMasterMessagesBlocking
  \item SendMasterWorkerMessagesBlocking
  \item ReceiveMasterMessagesBlocking
  \item SendAndReceiveLoadBalancingMessagesBlocking
  \item ReceiveIterationControlMessagesBlocking
  \item BroadcastToIdleNodesBlocking
  \item BroadcastToWorkingNodesBlocking
  \item SendHeapMetaDataBlocking
  \item SendAndReceiveHeapSynchronousDataBlocking
\end{itemize}  



\subsection*{MPI+X}

\begin{center}
 \begin{tabular}{p{2cm}p{2cm}p{8cm}l}
  Compile flag (default) & Alternative & Description & Target \\
  \hline
%   {\footnotesize MPI\-Progression\-Relies\-On\-MPI\-Test} 
%   & 
%   {\footnotesize noMPI\-Progression\-Relies\-On\-MPI\-Test} 
%   &
%   Some MPI implementations struggle to transfer data in the background of the
%   simulation run. In this case, one has to call \texttt{MPI\_Test} over an over
%   again. Each test call allows MPI to progress some messages. Obviously,
%   these calls do not come for free and impose some overhead, too. The flag
%   allows users to disable/enable this test polling. Unless not pinned
%   explicitly to one thread (see below), Peano's hybrid implementation
%   distributes the tests to the cores, i.e.~hardware threads call 
%   \texttt{MPI\_Test} whenever they become idle.
%   & MPI
  \\
  \hline
 \end{tabular}
\end{center}

%

\noindent
Please note that all parameters are written without hyphens, i.e.~a parameter
alike \texttt{noTrippleX} which is written as no-Tripple-X in the table above is
switched on on the compiler level through \texttt{-DTrippleX} and disabled
through \texttt{-DnoTrippleX}.




