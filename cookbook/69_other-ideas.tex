\section{Compiler-based optimisation}

\chapterDescription
  {
    5 minutes
  }
  {
  }


Peano's implementation can be tailed by tons of different compile options. 
At the end of the day, one might have to tailor them on a per-project base. 
As the settings first of all are tied to a particular machine (besides an
application), Peano collects all magic variables within a file called
{\em CompilerSpecificSettings.h}.
For some default platforms, we provide meaningful results.
All of these settings however can be overwritten no the command line.


\subsection*{Single core}

\begin{center}
 \begin{tabular}{p{2cm}p{2cm}p{9.5cm}}
  Compile flag (default) & Alternative & Description \\
  \hline
  {\footnotesize Use\-Manual\-Inlining} 
  & 
  {\footnotesize noUse\-Manual\-Inlining} 
  &
  Compilers are known to implement good heuristics which routines to inline and
  which not to inline. We however found that some compiler struggle with some
  parts of Peano. For these parts, we have added compiler-specific annotations
  that force the compiler to inline. Obviously, the identification of critical
  code parts has been done for few benchmarks only and it might happen that they 
  are invalid or even contra-productive for some applications. 
  \\
  \hline
 \end{tabular}
\end{center}
  

\subsection*{Shared memory}

\begin{center}
 \begin{tabular}{p{2cm}p{2cm}p{8cm}l}
  Compile flag (default) & Alternative & Description & Target \\
  \hline
  {\footnotesize UseTBBs\-Parallel\-For\-And\-Reduce} 
  & 
  {\footnotesize noUse\-TBBs\-Parallel\-For\-And\-Reduce} 
  &
  Different to TBB, Peano's internal task/job system literally is based on
  tasks only. That is, also parallel loops are mapped onto job spawning. For
  TBB-based codes, we found that such a realisation is sometimes disadvantageous
  for larger core counts. The flag allows us to toggle the implementation. 
  & TBB
  \\
  \hline
 \end{tabular}
\end{center}
  
  
  
\subsection*{Distributed memory}

\begin{center}
 \begin{tabular}{p{2cm}p{2cm}p{9.5cm}l}
  Compile flag (default) & Alternative & Description \\
  \hline
  {\footnotesize MPI\-Progression\-Relies\-On\-MPI\-Test} 
  & 
  {\footnotesize noMPI\-Progression\-Relies\-On\-MPI\-Test} 
  &
  Some MPI implementations struggle to transfer data in the background of the
  simulation run. In this case, one has to call \texttt{MPI\_Test} over an over
  again. Each test call allows MPI to progress some messages. Obviously,
  these calls do not come for free and impose some overhead, too. The flag
  allows users to disable/enable this test polling. Unless not pinned
  explicitly to one thread (see below), Peano's hybrid implementation
  distributes the tests to the cores, i.e.~hardware threads call 
  \texttt{MPI\_Test} whenever they become idle.
  \\
  \hline
 \end{tabular}
\end{center}


\noindent
No following variables are by default set to 0. 
  
  
%            /**
%           * @param communicateSleep -1 Data exchange through blocking mpi
%           * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
%           * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
%           */
  
  
%   #ifndef SendWorkerMasterMessagesBlocking
%  #define SendWorkerMasterMessagesBlocking     0
% #endif
% #ifndef SendMasterWorkerMessagesBlocking
%  #define SendMasterWorkerMessagesBlocking     0
% #endif
% #ifndef ReceiveMasterMessagesBlocking
%  #define ReceiveMasterMessagesBlocking        0
% #endif
% #ifndef SendAndReceiveLoadBalancingMessagesBlocking
%  #define SendAndReceiveLoadBalancingMessagesBlocking    0
% #endif
% #ifndef ReceiveIterationControlMessagesBlocking
%  #define ReceiveIterationControlMessagesBlocking        0
% #endif
% #ifndef BroadcastToIdleNodesBlocking
%  #define BroadcastToIdleNodesBlocking                   0
% #endif
% #ifndef BroadcastToWorkingNodesBlocking
%  #define BroadcastToWorkingNodesBlocking                0
% #endif
% #ifndef SendHeapMetaDataBlocking
%  #define SendHeapMetaDataBlocking                       0
% #endif
% #ifndef SendAndReceiveHeapSynchronousDataBlocking
%  #define SendAndReceiveHeapSynchronousDataBlocking      0
% #endif
%   
\subsection*{MPI+X}

\begin{center}
 \begin{tabular}{p{2cm}p{2cm}p{8cm}l}
  Compile flag (default) & Alternative & Description & Target \\
  \hline
%   {\footnotesize MPI\-Progression\-Relies\-On\-MPI\-Test} 
%   & 
%   {\footnotesize noMPI\-Progression\-Relies\-On\-MPI\-Test} 
%   &
%   Some MPI implementations struggle to transfer data in the background of the
%   simulation run. In this case, one has to call \texttt{MPI\_Test} over an over
%   again. Each test call allows MPI to progress some messages. Obviously,
%   these calls do not come for free and impose some overhead, too. The flag
%   allows users to disable/enable this test polling. Unless not pinned
%   explicitly to one thread (see below), Peano's hybrid implementation
%   distributes the tests to the cores, i.e.~hardware threads call 
%   \texttt{MPI\_Test} whenever they become idle.
%   & MPI
  \\
  \hline
 \end{tabular}
\end{center}

%

\noindent
Please note that all parameters are written without hyphens, i.e.~a parameter
alike \texttt{noTrippleX} which is written as no-Tripple-X in the table above is
switched on on the compiler level through \texttt{-DTrippleX} and disabled
through \texttt{-DnoTrippleX}.




