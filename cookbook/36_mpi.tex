\chapter{MPI and shared memory programming}
\label{section:mpi}

If you run Peano4 with MPI, you buy into MPI's SPMD paradigm, i.e.~every single
rank hosts one instance of Peano.
Each Peano instance in return hosts multiple subspacetrees, i.e.~multiscale
domain partitions.
It is the user's responsibility to ensure that the ranks do coordinate with each
other.
That is, the user has to ensure that whenever you run a certain type of grid
sweep on one rank, then the other ranks run this sweep as well.
For this, each rank has to host one spacetree set, and these sets have to be
configured with the same parameters (domain offset plus domain size).
I do provide Python interfaces/APIs to model this behaviour, but it might be
reasonable to understand how it works.



A typical Peano4 code distinguishes the global master (rank 0) from the workers
(all other ranks).
The global master hosts all the program logic, i.e.~decides which steps to run
in which order.
There's no reason for you not to emancipate from this pattern, but it has proven
of value.
The main of this pattern always looks similar to 
\begin{code}
  if (tarch::mpi::Rank::getInstance().isGlobalMaster() ) {
    // All the program logic, i.e. all decisions here
  }
  else {
    // React to decisions what to do next on all other ranks
  }
\end{code}


Before we start, lets assume that each individual step (type of run through the
mesh) has a unique positive number. 

A typical parallel grid sweep looks as follows:
\begin{enumerate}
  \item 
\end{enumerate} 



\section{Realising a domain decomposition}

This is a howto for domain decomposition. 
For details about the underlying algorithmics and design decisions, please
consult Chapter \ref{section:design:domain-decomposition}.
As mentioned in various places, Peano does not fundamentally distinguish shared
memory and distributed memory parallelisation unless you work explicitly with
tasks.
That is, all split remarks here do also apply to shared memory.


Peano's core domain decomposition contact point is the singleton
\texttt{SpacetreeSet} on each compute node (rank).
You can tell this set to decompose one of the local spacetrees further through
its \texttt{split} routine.
After each split, the involved spacetrees need at least two iterations to
``recover'' before they are available for further spilts.


Split accepts the number of fine grid cells that are two be given away to
another rank.
The domain decomposition follows the Peano space-filling curve.
If $3^d$ cells along this curve are to be deployed to a different rank, then
Peano also deploys the parent of this $3^d$ assembly to a remote rank.
With dynamic AMR (or throughout an initial grid construction), it can happen
that Peano runs into a cell that is to be moved over to another rank and is
refined at the same time.
In this case, the cell plus all of its children is deployed.


\begin{remark}
 Splits imply that some spacetree data has to be copied and (temporarily) is
 held redundantly. In principle, you can issue as many splits in one rush as you
 like. In practice, it often makes sense to constrain the maximum number of
 splits issued per grid traversal. This ensures that you don't run out of
 memory.
\end{remark}



The issue with (real) dynamic AMR is that it is very difficult to know how many
cells will be around after a grid sweep.
The same holds for the grid construction.
For runtime and memory reasons, you don't want to build up the grid completely
before you start to decompose it.
The following realisation pattern has turned out to be advantageous:
\begin{itemize}
  \item Build up an initial grid on the first rank up to a certain, reasonable,
  grid level. It should host at least $3^d \cdot r$ fine grid cells if you have
  $r$ ranks. This seems to be a reasonable rule of thumb.
  \item Split up this initial grid into $r$ spacetrees where each spacetrees is
  associated with a node of its own.
  \item Wait for one tree traversal, i.e.~run through the grid without adding
  further mesh elements. This ensures that the distributed mesh has been
  migrated completely before we continue.
  \item Continue to build up the grid.
  \item Compute a good guess how many cells should roughly end up per core.
  Store this guess.
  \item While the grid is constructed, decompose it further.
  \item Once the grid is complete, recompute how many cells should end up per
  core. Split up all the trees that are too big further and distribute these
  further trees among the ranks.
\end{itemize}

 
