\chapter{A geometric-algebraic multigrid solver}
\label{section:python-api-examples:multigrid}

All the examples from this chapter can be found in Peano's
\texttt{python/examples} directory.


\section{A matrix-free Jacobi smoother with rediscretisation}

\begin{remark}
 This example corresponds to the \texttt{matrix-free.py} script.
\end{remark}



\section{A parallel version of the code}

\begin{remark}
 This example corresponds to the \texttt{amr-parallel.py} script.
\end{remark}

For the time being, we again keep stuff pretty simple.
We stick to a regular grid, create this grid serially on one rank or core, 
and then distribute it.
That is, we don't do any load balancing and, as long as the grid remains 
stationary, we will have a brilliant balancing.
For the remainder, the text will use rank/core/partition as synonyms.
Peano 4 does not distinguish between them, i.e.~it uses domain decomposition
to keep both ranks and threads busy.


The first step to integrate multicore parallelisation---I always recommend to
start with multithreading first and then to add MPI on top---is to set an
appropriate number of threads.
Different to OpenMP, Peano requires the user to configure the thread count
within the code. 
This is done via the configuration method of the singleton
\texttt{tarch::multicore::Core}\footnote{Peano's C++14 multithreading variant
at the moment does not allow you to set the thread count, as C++ offers no such
feature. I therefore recommend TBB.}.


To split up a tree, you have to call
\begin{code}
  peano4::parallel::SpacetreeSet::getInstance().split(mytreeNumber,cellsPerCore,destinationRank)
\end{code}
\noindent
on your rank. This operation wants to know which spacetree to split, how many
cells to cut from it (we always count fine grid cells), and to which rank to
deploy the split tree.
The operation establishes a logical tree topology on the spacetrees, as we
always start with tree number 0 on rank 0 and then fork and fork and fork.
Joins are currently not supported by Peano 4's API: the code automatically fuses
trees if they degenerate.


After each split, you need at least two iterations to actually realise it, as 
Peano has to move the data, update all data meta information, and so forth. 
The iterations (grid sweeps) are no special ones, i.e.~you can continue to
compute.
It is just reasonable to keep in mind that after a split of a rank, this rank
will not be able to do any further domain decomposition for a few more sweeps
until it has ``recovered''. 
Likewisely, new trees will not enter the game immediately, but with a 1--2
iterations delay.
If you want to avoid that computations and domain decomposition intermix, you
can add code snippets alike

\begin{code}
int cellsPerCore = peano4::parallel::SpacetreeSet::getInstance().getGridStatistics().getNumberOfLocalUnrefinedCells() / coreCount;
logInfo( "main()", "should host " << cellsPerCore << " cells per core" );

for (int thread=1; thread<coreCount; thread++) {
  if ( not peano4::parallel::SpacetreeSet::getInstance().split(0,cellsPerCore,0)) { 
   logWarning( "runParallel(...)", "failed to assign thread " << thread << " " << cellsPerCore << " cell(s)" ); 
  }
}
\end{code}

\noindent
This is however your decision. Peano does not impose any constraints on you that
you have to separate load balancing, load re-distribution and computations.


\section{A parallel AMR Jacobi solver}

\begin{remark}
 This example corresponds to \texttt{amr-parallel.py} script, too.
\end{remark}
