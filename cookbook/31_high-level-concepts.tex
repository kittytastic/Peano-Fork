\chapter{Guiding principles, high-level concepts and design decisions}


\section{Generic structure of a \Peano\  application}

Though I do not tell users exactly how to write their code, there's some kind of
generic structure that is common to all Peano simulations.
We can consider it to be some kind of pattern/best practice.


First, we distinguish the global master---typically rank 0---and all the other
ranks.
It is the global master that makes all the decisions and hosts the main logic.
It tells the other ranks what to do in each step. 
Typical actions are
\begin{itemize}
  \item run through your mesh once,
  \item split up the domain (though each rank can decide to do this totally
  independently),
  \item plot some data, or, eventually,
  \item terminate.
\end{itemize}


Therefore, the global master basically sets a counter/id which uniquely
identifies which step to run next.
It then says ``go ahead'' and runs this step itself.
All the other ranks are noticed (via an internal broadcast in the core) which
step to run, do the same thing, and then wait for the next wake-up call.
At the end, the global master sets the id of a terminate.
As all the ranks loop until they receive this terminate id, they do go down as
well in the end.


A typical main thus looks as follows
\begin{code}
  if (tarch::mpi::Rank::getInstance().isGlobalMaster() ) {
    while ( selectNextAlgorithmicStep() ) {
      step();
    }
  }
  else {
    while (peano4::parallel::Node::getInstance().continueToRun()) {
      step();
    }
  }
\end{code}


\noindent
where the routine \texttt{selectNextAlgorithmicStep()} issues one call of
\begin{code}
peano4::parallel::Node::getInstance().setNextProgramStep(myStepNumber);
\end{code}

The \texttt{step()} routine then is usually a big switch statement:
\begin{code}
int stepIdentifier = peano4::parallel::Node::getInstance().getCurrentProgramStep();
switch (stepIdentifier) {
  case 0:
    {
      observers::CreateGrid  observer;
      peano4::parallel::SpacetreeSet::getInstance().traverse(observer);
    }
    break;
  case 1:
    {
      // split up domain
    }
}
\end{code}


\noindent
Nothing stops you from triggering multiple traversals (or none at all) within a
step.


\begin{remark}
 On the highest level, Peano4 implements a BSPish structure where the global
 rank issues a step and then all ranks run this step simultaneously. You can
 however define the notion of a superstep, i.e.~make one superstep issue
 multiple grid traversals. It is just important that all spacetree sets do
 exactly the same number of iterations.
\end{remark}


\noindent
The numbers within the case statements are kind of magic constants and you can
redefine them. 
However, I recommend to read the documentation of \texttt{setNextProgramStep()}
(there are some predefined constants that you should not use), and I recommend
that you use the predefined enums in \texttt{observers::StepRepository} if you
work with the Python API, too.
In this repository, there are predefined integers for all of the algorithm steps
you've defined. 


With the above structure, the Peano execution trace is close to trivial:
You simply run through your grid $n$ times and after each step, the core
exchanges all of the data.


\section{MPI and shared memory programming}

If you run Peano4 with MPI, you buy into MPI's SPMD paradigm, i.e.~every single
rank hosts one instance of Peano.
Each Peano instance in return hosts multiple subspacetrees, i.e.~multiscale
domain partitions.
It is the user's responsibility to ensure that the ranks do coordinate with each
other.
That is, the user has to ensure that whenever you run a certain type of grid
sweep on one rank, then the other ranks run this sweep as well.
For this, each rank has to host one spacetree set, and these sets have to be
configured with the same parameters (domain offset plus domain size).
I do provide Python interfaces/APIs to model this behaviour, but it might be
reasonable to understand how it works.



A typical Peano4 code distinguishes the global master (rank 0) from the workers
(all other ranks).
The global master hosts all the program logic, i.e.~decides which steps to run
in which order.
There's no reason for you not to emancipate from this pattern, but it has proven
of value.
The main of this pattern always looks similar to 
\begin{code}
  if (tarch::mpi::Rank::getInstance().isGlobalMaster() ) {
    // All the program logic, i.e. all decisions here
  }
  else {
    // React to decisions what to do next on all other ranks
  }
\end{code}


Before we start, lets assume that each individual step (type of run through the
mesh) has a unique positive number. 

A typical parallel grid sweep looks as follows:
\begin{enumerate}
  \item 
\end{enumerate} 



\subsection{Realising a domain decomposition}

This is a howto for domain decomposition. 
For details about the underlying algorithmics and design decisions, please
consult Chapter \ref{section:design:domain-decomposition}.
As mentioned in various places, Peano does not fundamentally distinguish shared
memory and distributed memory parallelisation unless you work explicitly with
tasks.
That is, all split remarks here do also apply to shared memory.


Peano's core domain decomposition contact point is the singleton
\texttt{SpacetreeSet} on each compute node (rank).
You can tell this set to decompose one of the local spacetrees further through
its \texttt{split} routine.
After each split, the involved spacetrees need at least two iterations to
``recover'' before they are available for further spilts.


Split accepts the number of fine grid cells that are two be given away to
another rank.
The domain decomposition follows the Peano space-filling curve.
If $3^d$ cells along this curve are to be deployed to a different rank, then
Peano also deploys the parent of this $3^d$ assembly to a remote rank.
With dynamic AMR (or throughout an initial grid construction), it can happen
that Peano runs into a cell that is to be moved over to another rank and is
refined at the same time.
In this case, the cell plus all of its children is deployed.


\begin{remark}
 Splits imply that some spacetree data has to be copied and (temporarily) is
 held redundantly. In principle, you can issue as many splits in one rush as you
 like. In practice, it often makes sense to constrain the maximum number of
 splits issued per grid traversal. This ensures that you don't run out of
 memory. Also, splits require the Peano core to run some steps serially. 
 So also from a time-to-solution point of view, limiting the number of splits
 (in particular if they affect the same rank) is a good idea.
\end{remark}



The issue with (real) dynamic AMR is that it is very difficult to know how many
cells will be around after a grid sweep.
The same holds for the grid construction.
For runtime and memory reasons, you don't want to build up the grid completely
before you start to decompose it.
The following realisation pattern has turned out to be advantageous:
\begin{itemize}
  \item Build up an initial grid on the first rank up to a certain, reasonable,
  grid level. It should host at least $3^d \cdot r$ fine grid cells if you have
  $r$ ranks. This seems to be a reasonable rule of thumb.
  \item Split up this initial grid into $r$ spacetrees where each spacetrees is
  associated with a node of its own.
  \item Wait for one tree traversal, i.e.~run through the grid without adding
  further mesh elements. This ensures that the distributed mesh has been
  migrated completely before we continue.
  \item Continue to build up the grid.
  \item Compute a good guess how many cells should roughly end up per core.
  Store this guess.
  \item While the grid is constructed, decompose it further.
  \item Once the grid is complete, recompute how many cells should end up per
  core. Split up all the trees that are too big further and distribute these
  further trees among the ranks.
\end{itemize}



\begin{remark}
Irgendwie darf man net 2x traverse aufrufen in meinem Code. Evtl. liegt das aber
auch am Datenaustausch. Aber ist halt einfach bad smell in meinem Code!
\end{remark}

 

\subsection{Global communication}

\Peano\ handles all (multiscale) boundary data transfer and data migration
within the mesh.
Everything not tied to the mesh is \emph{not} handled by \Peano.
That means there's not even a reduction of the grid statistics over MPI.
We do provide reduction and broadcast features however. 

 