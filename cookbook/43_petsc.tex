\section{Diffusion-convection with PETSc}
  \label{section:applications:petsc}

\chapterDescription
  {
    60 minutes.
  }
  {
    Chapter \ref{chapter:quickstart} and a working PETSc installation.
  }

I wrote Peano with matrix-free methods from CFD in mind. 
It turns out that it is not too complicated to use it for other application
areas or to established linear algebra packages.
We assume that you have PETSc plus BLAS, Lapack and MPI (if required) ready
installed on your system.
I have tested all the present concepts with PETSc 3.7.2 but everything discussed
here is pretty basic, so it should work with older versions as well.
Please note that this is a feasibility study and we do not optimise at all.


\subsection{Preparation of Peano}

If we use explicit assembly, we need an enumeration of our grid. 
So we do start with a grid
\begin{code}
> java -jar <mypath>/pdt.jar --create-project petsc chapter-4.3
\end{code}

\noindent
and add each vertex an index. 
We do restrict to a discretisation with one degree of freedom per vertex for the
time being:

\begin{code}
Packed-Type: short int;

class petsc::records::Vertex {  
  parallelise persistent int index;
};
\end{code}

\noindent
For the present exercise, we will have four different things to do:
create the grid, create and assemble the matrix, solve the problem and plot the
result.
The solve is a pure PETSc operation.
For each of the remaining three phases, we need a mapping/adapter in Peano and
we also want access to \texttt{index}.
For the latter, we could write setters and getters and stick to an
object-oriented paradigm.
But this way, it is easier (though not as beautiful):

\begin{code}
component: MyFancyPETScProject

namespace: ::petsc

vertex:
  dastgen-file: Vertex.def
  read scalar(int): Index   // mind the uppercase
  write scalar(int): Index
  
cell:
  dastgen-file: Cell.def

state:
  dastgen-file: State.def

event-mapping:
  name: CreateGrid

event-mapping:
  name: Assemble

adapter:
  name: CreateGrid
  merge-with-user-defined-mapping: CreateGrid

adapter:
  name: Assemble
  merge-with-user-defined-mapping: Assemble
 
adapter:
  name: Plot
  merge-with-predefined-mapping: VTKPlotVertexValue(result,getU,u)
\end{code}

We generate all glue code
\begin{code}
> java -jar <mypath>/pdt.jar --generate-gluecode \
  petsc/project.peano-specification petsc <mypath>/pdt/usrtemplates
> make -f petsc/makefile
\end{code}

\noindent
We recognise that this code does not compile yet as we have not yet realised \texttt{double Vertex::getU() const}.
For the time being, we make the routine return 0.
Finally, we change into the mapping the creational events yield a regular grid:

\begin{code}
void petsc::mappings::CreateGrid::createInnerVertex(...) {
  logTraceInWith6Arguments( "createInnerVertex(...)", ... );

  if (coarseGridVerticesEnumerator.getLevel()<3) {
	fineGridVertex.refine();
  }

  logTraceOutWith1Argument( "createInnerVertex(...)", fineGridVertex );
}


void petsc::mappings::CreateGrid::createBoundaryVertex(...) {
  logTraceInWith6Arguments( "createBoundaryVertex(...)", ... );

  if (coarseGridVerticesEnumerator.getLevel()<3) {
	fineGridVertex.refine();
  }

  logTraceOutWith1Argument( "createBoundaryVertex(...)", fineGridVertex );
}
\end{code}

\noindent
Compile it, run it, visualise the output.


\subsection{Connecting to PETSc}

As PETSc's philosophy is pretty straightforward and minimalistic (they do, e.g.,
not hijack the main), the initialisation and integration are straightforward.
Please note that we stick to serial jobs in the first part of this section.
Nevertheless, we use the MPI compiler wrapper to translate the code as I faced
issues when I tried to use PETSc without MPI\footnote{Please consult
\url{http://www.mcs.anl.gov/petsc/petsc-current/docs/installation.html#i-dont-want-to-use-mpi}
for the usage with OpenMPI as the \texttt{LD\_LIBRARY\_PATH} has to be set
properly.}.


\begin{enumerate}
  \item Before we start to code with PETSc, we have to make the makefile know
  where PETSc is installed. If you use your own build environment, adopt all
  pathes accordingly. If you use the pre-generated makefile, open it and add
  your PETSc include directory to the search path. Also switch to
  \texttt{mpiCC}.
  \begin{code}
    PROJECT_CFLAGS = -DDebug -DAsserts -DParallel \
      -DMPICH_IGNORE_CXX_SEEK -I/opt/petsc/include 
    PROJECT_LFLAGS = -L/opt/petsc/lib -lpetsc 
    PROJECT_LFLAGS = -L/opt/petsc/lib -lpetsc

    [...]

    CC=/opt/openmpi/bin/mpiCC
  \end{code}
  Please adopt all pathes accordingly. Depending on your MPI version, you also
  might run into issues with the compiler's error management. I recommend to
  remove \texttt{-Werror -pedantic-errors} from your makefile in this case.
  \item As the present PETSc solutions use MPI (through they run serially), we
  have to make a few modifications in the runner. These modifications are
  detailed in Chapter \ref{section:parallelisation:mpi}, so we simply show
  what has to be changed in \texttt{runners/Runner.cpp}. This solution should
  work out-of-the-box.
  For a description of its semantics, please see the aforementioned chapter.
  \begin{code}
#include "tarch/parallel/FCFSNodePoolStrategy.h"
#include "peano/parallel/loadbalancing/OracleForOnePhaseWithGreedyPartitioning.h"

[...]

int petsc::runners::Runner::run() {

  [...]

  petsc::repositories::Repository* repository = 
    petsc::repositories::RepositoryFactory::getInstance().createWithSTDStackImplementation(
      geometry,
      tarch::la::Vector<DIMENSIONS,double>(1.0),   // domainSize,
      tarch::la::Vector<DIMENSIONS,double>(0.0)    // computationalDomainOffset
    );
  

  // This is new because of PETSc:
  if (tarch::parallel::Node::getInstance().isGlobalMaster()) {
    tarch::parallel::NodePool::getInstance().setStrategy(
      new tarch::parallel::FCFSNodePoolStrategy()
    );
  }
  tarch::parallel::NodePool::getInstance().restart();
  tarch::parallel::NodePool::getInstance().waitForAllNodesToBecomeIdle();
  peano::parallel::loadbalancing::Oracle::getInstance().setOracle(
    new peano::parallel::loadbalancing::OracleForOnePhaseWithGreedyPartitioning(false)
  );

  [...]
}
  \end{code}
  \item Finally, we have to initialise and shut down PETSc properly in
  \texttt{main.cpp}:
  \begin{code}
#include "petscsys.h"

[...]

int main(int argc, char** argv) {
  [...]
  PetscInitialize(&argc,&argv,(char*)0,(char*)0);

  int programExitCode = 0;

  if (programExitCode==0) {
    [...]
    petsc::runners::Runner runner;
    programExitCode = runner.run();
  }

  PetscFinalize();
  [...]
}  
  \end{code}
  According to PETSc's documentation, we could use PETSc's initialisation to set
  up MPI instead of the Peano initialisation. However, Peano's initialisation
  does not check whether MPI has been set up before where PETSc does. So it
  makes sense to initialise Peano first.
\end{enumerate}




\subsection{Setting up the PETSc data structures}

Once we have created the grid (and perhaps visualised it), we can start to set
up the PETSc data structures corresponding to the grid. 
We stick to the regular case for the time being.

Every fine grid vertex in the grid has a right-hand side and a solution entry. 
Different to previous sections, we do not store this data in the vertex.
Instead, we will make PETSc hold two vectors $rhs$ and $x$ and
make each vertex hold the index, i.e.~each vertex knows which entry in the two
vertices is associated with the vertex.



@todo Discuss the whole Assemble.cpp as well as the header.

#include "petscvec.h"



\subsection{Serial assembly with a parallel PETSc}


\subsection{Using PETSc and MPI}
% others. So it is a perfect fit to Peano.

