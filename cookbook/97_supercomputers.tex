\chapter{Selected HPC platforms and tools}
\label{chapter:selected-HPC-platforms}


\section{SuperMUC-NG}

% Brauchen wir -DnoPackedRecords
% 
% \begin{code}
%  ssh -X lu57zat6@skx.supermuc.lrz.de
%  cd $WORK
%  export CXX icpc
% \end{code}
% So geht es auch auf dem Login-Knoten. Aber da gehen keine Tools.
% export I_MPI_HYDRA_BOOTSTRAP=fork

All Intel tools are properly preloaded on SuperMUC-NG.
Hence, we can directly translate the code with MPI. 
However, TBB seems not to be a standard package within the Intel module anymore. 
So if you need TBB, you have to load it manually.
LRZ's module environment sets all pathes and settings coming along with TBB in
environment variables \texttt{TBB\_INC} and \texttt{TBB\_SHLIB}.
To pass these variables into Peano, map them onto \texttt{CXXFLAGS} or
\texttt{LDFLAGS}.

\begin{code}
 module load tbb
 export CXXFLAGS="$TBB_INC"
 export LDFLAGS="$TBB_SHLIB"
 ./configure --with-multithreading=tbb --with-mpi=mpiicpc
 make clean
 make -j
\end{code}


\noindent
Here's the non-MPI variant:

\begin{code}
 module load tbb
 export CXX=icpc
 export CXXFLAGS="$TBB_INC"
 export LDFLAGS="$TBB_SHLIB"
 ./configure --with-multithreading=tbb
 make clean
 make -j
\end{code}


\noindent
If you want to use the legacy DaStGen variant with Java, you have to load
Java manually.
\ExaHyPE\ for example does not need this legacy version anymore.
Python3 in contrast is preloaded by default.


\begin{remark}
 I found it reasonable to always deactivate the energy environment by the LRZ.
 In particular if you combine Peano4 with teaMPI or the Intel tools, the energy
 tools don't work anymore, as they plug into the same MPI interface (PMPI),
 i.e.~they compete for the same hook-in points. So my SLURM scripts always
 contain the line
 \begin{code}
#SBATCH --ear=off
module load slurm_setup
 \end{code}
 Please note that load of the package \texttt{slurm\_setup} which seems to be
 required, too.
 Furthermore, LRZ recommends that you use \texttt{mpiexec} rather than
 \texttt{srun}.
\end{remark}



\section{AMD MI50 test cluster}


\begin{code}
./configure --enable-exahype CXX="clang++" LDFLAGS="-lm" 
\end{code}


\section{Hamilton}

%  ssh frmh84@hamilton.dur.ac.uk
%  cd $SCRATCH

\begin{code}
 module purge
 module load intel/2019.5
 module load intelmpi/intel/2019.6
 # for the Python API
 module load python/3.6.8 
 module unload gcc/8.2.0
 module load gcc/9.3.0
 # for legacy DaStGen
 # module load java/1.8.0
 setenv CXX mpicxx
 setenv CXXFLAGS "-DLogService=ITACLogger -I/ddn/apps/Cluster-Apps/intel/2019.5/tbb/include"
 setenv LDFLAGS "-L/ddn/apps/Cluster-Apps/intel/2019.5/tbb/lib/intel64/gcc4.7 -ltbb"
 ./configure --with-multithreading=tbb --with-mpi=mpiicpc
 make clean
 make -j
\end{code}



% ./configure --with-multithreading=omp --with-intel


\begin{remark}
 It is confirmed that the Python module should not load gcc, and that this will
 be removed in future installations.
 In the meantime, the unload is required.
 If you skip it, then the Intel linker will fail.
\end{remark}

\noindent
The Java module is again required if and only if you use the legacy DaStGen
feature to model data structures.
If you decide to use the C++ multithreading, you can leave out the TBB-specific
settings.


For Intel's ITAC, we have to load the itac module and reconfigure:
\begin{code}
module itac/2020.2
setenv CXXFLAGS "-DLogService=ITACLogger -I/ddn/apps/Cluster-Apps/intel/2019.5/tbb/include -I"$ITAC_HOME"/itac_2020/intel64/include"
./configure ... --with-intel 
\end{code}



\section{COSMA/DINE (Durham Intelligent NIC Environment)}


Log into the first DINE node as by documentation (\texttt{ssh -X b101}) and run

\begin{code}
module load intel_comp/2020-update1
module load intel_mpi/2020
module unload python/2.7.15
module load python/3.6.5
./configure --with-multithreading=omp --with-mpi=mpiicpc
\end{code}



\section{Local workstations}


If you don't have a proper module file, you might have to configure your environment manually to use TBB:
\begin{code}
 export CXXFLAGS=-I/opt/intel/tbb/include
 export LDFLAGS="-L/opt/intel/tbb/lib/intel64/gcc4.7 -ltbb -pthread"
 ./configure --with-multithreading=tbb --with-mpi=/opt/mpi/mpicxx
\end{code}


If you want to build against VTK, you have to set the pathes accordingly. 
On my machine, it looks similar to
\begin{code}
 export CXXFLAGS=-I/opt/vtk/include/vtk-9.0
 export LDFLAGS=-L/opt/vtk/lib64
 ./configure --with-vtk=-9.0
\end{code}


With the Intel tools, you have to ensure that the environment is set up 
properly:
\begin{code}
source /opt/intel/bin/iccvars.sh intel64
source /opt/intel/itac/2020.0.015/bin/itacvars.sh
source /opt/intel/impi/2019.6.166/intel64/bin/mpivars.sh
\end{code}
\label{label:supercomputer:Intel-scripts}


If I wanna use the Fortran/MHD stuff, I need an additional
\begin{code}
source /opt/intel/bin/ifortvars.sh intel64
\end{code}
to make the compiler \texttt{ifort} available to \Peano.


\section{The Intel tools}
\label{section:supercomputers:IntelTools}

\paragraph{Intel's Threading Building Blocks (TBB)}

The above descriptions all include the TBB tools in the release version.
If you want to debug with TBB or to use the Intel tools, then I recommend to
link against \texttt{tbb\_debug} and to set a few extra command line arguments:

\begin{code}
 export PEANO_CXXFLAGS="-DTBB_USE_ASSERT -DTBB_USE_THREADING_TOOLS "
 export LDFLAGS="see-above   -ltbb_debug"
\end{code}

\noindent
Some systems such as SuperMUC-NG provide a dedicated debug environments,
i.e.~here you can use
\begin{code}
 export CXXFLAGS="$TBB_INC -DTBB_USE_ASSERT -DTBB_USE_THREADING_TOOLS "
 export LDFLAGS="$TBB_SHLIB_DEBUG"
\end{code}


\paragraph{Intel Trace Analyser and MPI Correctness Checks}

If you want to use the Intel tracing or the correctness checks, you have to
translate the code with the linker flags \texttt{-trace} or \texttt{check\_mpi},
respectively.
To set them, please use the \texttt{PEANO\_LDFLAGS}.
If you try to overwrite \texttt{LDFLAGS}, configure sometimes complains.


\begin{code}
 export PEANO_LDFLAGS=-trace
 export PEANO_LDFLAGS=-check_mpi
 ./configure --with-multithreading=tbb --with-mpi=mpiicpc
 make clean
 make -j
\end{code}


\noindent
As Peano consists of multiple libraries which are (partially) linked before you
include them into your executable, you have to rebuild the whole project again.
If you don't do this, you will get error messages that data is corrupted or MPI
datatypes are not defined.



\begin{remark}
 I had crashes in the code as soon as I switched
 \begin{code}
 export VT_CHECK_TRACING=on
 \end{code}
 on. So this seems not to work with Peano.
\end{remark}



\begin{remark}
 I had deadlocks in my code with
 \begin{code}
 export LD_PRELOAD=libVTfs.so
 \end{code}
 as soon as I did remove the \texttt{-check\_mpi} flag from \texttt{mpirun}.
\end{remark}



\begin{remark}
 I have not yet succeeded in running Intel's \texttt{-check\_mpi} with \ExaHyPE.
 The checker claims that the code deadlocked, even though it does not. There
 seems to be a problem with the tool (2019 version) and the nonblocking barrier
 used in \ExaHyPE\ to synchronise all ranks.
\end{remark}


\section{Using MUST}
\label{section:supercomputers:MUST}

Using MUST is straightfoward with Peano:
\begin{code}
  export PATH=/usr/local/must/bin/:$PATH
\end{code}


\noindent
You have to be careful however not to use the \texttt{-check\_mpi} flag with the
Intel tools if you want to run MUST.
With the Intel checker, MUST does not produce any output.
Once the MUST path is set properly, you replace the \texttt{mpirun} with
\texttt{mustrun}.
We've checked all core components of Peano to pass through MUST without any
problems.


% 
% \paragraph{Intel MPI}
% 
% If you you Intel MPI, it seems that you now have to add the \texttt{-parallel}
% flag to your makefile if you use the Python API. Otherwise, your linker will
% fail.
% I haven't yet found out which ``misconfiguration'' in Peano4 causes this
% dependency.
% As the flag is not required on other systems, the Python's API output does not
% by default add this field.
% So either modify the makefile manually or set it as additional \texttt{LDFLAGS}
% entry before you compile.
% The core b.t.w.~seems to be fine without the flag for whatever reason.

 
% nimm das -openmp raus aus dem Script, dann braucht es das -parallel im Linker net
% 
% -mt_mpi
% 
% 
% mpiexec is hte only support thing. no mpirun no srun
% 
% https://doku.lrz.de/display/PUBLIC/Job+Processing+with+SLURM+on+SuperMUC-NG
% 
% 
% 
% 
% https://software.intel.com/en-us/application-snapshot-user-guide-creating-configuration-file-for-intel-trace-collector
% 
% https://doku.lrz.de/display/PUBLIC/Intel+Tracing+Tools%3A+Profiling+and+Correctness+checking+of+MPI+programs#IntelTracingTools:ProfilingandCorrectnesscheckingofMPIprograms-ConfigurationFile
% 
% 
% 
% 
