\chapter{Selected HPC platforms and tools}
\label{chapter:selected-HPC-platforms}


\section{SuperMUC-NG}

% Brauchen wir -DnoPackedRecords
% 
% \begin{code}
%  ssh -X lu57zat6@skx.supermuc.lrz.de
%  cd $WORK
%  export CXX icpc
% \end{code}
% So geht es auch auf dem Login-Knoten. Aber da gehen keine Tools.
% export I_MPI_HYDRA_BOOTSTRAP=fork

All Intel tools are properly preloaded on SuperMUC-NG.
Hence, we can directly translate the code with MPI. 
However, TBB seems not to be a standard package within the Intel module anymore. 
So if you need TBB, you have to load it manually.
LRZ's module environment sets all pathes and settings coming along with TBB in
environment variables \texttt{TBB\_INC} and \texttt{TBB\_SHLIB}.
To pass these variables into Peano, map them onto \texttt{CXXFLAGS} or
\texttt{LDFLAGS}.

\begin{code}
 module load tbb
 export CXXFLAGS="$TBB_INC"
 export LDFLAGS="$TBB_SHLIB"
 ./configure --with-multithreading=tbb --with-mpi=mpiicpc
 make clean
 make -j
\end{code}


\noindent
Here's the non-MPI variant:

\begin{code}
 module load tbb
 export CXX=icpc
 export CXXFLAGS="$TBB_INC"
 export LDFLAGS="$TBB_SHLIB"
 ./configure --with-multithreading=tbb
 make clean
 make -j
\end{code}


\noindent
If you want to use the legacy DaStGen variant with Java, you have to load
Java manually.
\ExaHyPE\ for example does not need this legacy version anymore.
Python3 in contrast is preloaded by default.


\begin{remark}
 I found it reasonable to always deactivate the energy environment by the LRZ.
 In particular if you combine Peano4 with teaMPI or the Intel tools, the energy
 tools don't work anymore, as they plug into the same MPI interface (PMPI),
 i.e.~they compete for the same hook-in points. So my SLURM scripts always
 contain the line
 \begin{code}
#SBATCH --ear=off
module load slurm_setup
 \end{code}
 Please note that load of the package \texttt{slurm\_setup} which seems to be
 required, too.
 Furthermore, LRZ recommends that you use \texttt{mpiexec} rather than
 \texttt{srun}.
\end{remark}



\section{AMD MI50 test cluster}


The HIP compiler can't do OpenMP offloading at the moment, i.e.~a simple
translation with hipcc does not work. 
Instead, we use the command \texttt{hipconfig --cpp\_config} to identify HIP's
compiler settings and then run the code.


Simple clang++ should translate the code for the host:
\begin{code}
./configure --enable-exahype CXX="clang++" LDFLAGS="-lm" 
./configure --enable-exahype --with-multithreading=omp CXX="clang++ CXXFLAGS="-DnoGPUOffloading" LDFLAGS="-lm"
\end{code}

% The HIP compiler is not yet able to offload computations to the GPU:
% \begin{code}
% ./configure --enable-exahype --with-multithreading=omp CXX="clang++" \
%   CXXFLAGS="-D__HIP_PLATFORM_HCC__=  -I/opt/rocm-3.5.1/hip/include -I/opt/rocm-3.5.1/llvm/bin/../lib/clang/11.0.0 -I/opt/rocm-3.5.1/hsa/include -D__HIP_ROCclr" \
%   LDFLAGS="-lm"
% \end{code}

% If I use the clang compiler from rocl, the stuff does not yet work:
% \begin{code}
% ./configure --enable-exahype --with-multithreading=omp \
%  CXX="/opt/rocm-3.5.0/aomp/bin/clang++" \
%  CXXFLAGS="-D__HIP_PLATFORM_HCC__=  -I/opt/rocm-3.5.1/hip/include -I/opt/rocm-3.5.1/llvm/bin/../lib/clang/11.0.0 -I/opt/rocm-3.5.1/hsa/include -D__HIP_ROCclr -I/opt/rocm-3.5.0/aomp/include/ -fopenmp" \
%  LDFLAGS="-lm -fopenmp"
% \end{code}


\noindent
Therefore, we rely on the fact that ROCM is installed at \texttt{/opt/rocm} and
configure as follows:

\begin{code}
./configure --enable-exahype --with-multithreading=omp \
 CXX="/opt/rocm/aomp/bin/clang++" \
 CXXFLAGS="-target x86_64-pc-linux-gnu -fopenmp -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx906" \
 LDFLAGS="-lm -fopenmp"
\end{code}


% ./configure --enable-exahype --with-multithreading=omp \
%  CXX="/opt/rocm/aomp/bin/clang" \
%  CXXFLAGS="-target x86_64-pc-linux-gnu -fopenmp -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx906" \
%  LDFLAGS="-lm -fopenmp"



% ./configure --enable-exahype --with-multithreading=omp \
%  CXX="/opt/rocm/aomp/bin/clang++" \
%  CXXFLAGS="-target x86_64-pc-linux-gnu -fopenmp -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx906 --hip-device-lib-path=/opt/rocm-3.5.1/lib" \
%  LDFLAGS="-lm -fopenmp"






\section{Hamilton}

%  ssh frmh84@hamilton.dur.ac.uk
%  cd $SCRATCH

\begin{code}
 module purge
 module load intel/2019.5
 module load intelmpi/intel/2019.6
 module load python/3.6.8 
 module unload gcc/8.2.0
 module load gcc/9.3.0
 # for legacy DaStGen (usually not required)
 # module load java/1.8.0
 setenv CXX mpicxx
 setenv CXXFLAGS "-DLogService=ITACLogger -I/ddn/apps/Cluster-Apps/intel/2019.5/tbb/include"
 setenv LDFLAGS "-L/ddn/apps/Cluster-Apps/intel/2019.5/tbb/lib/intel64/gcc4.7 -ltbb"
 ./configure --with-multithreading=tbb --with-mpi=mpiicpc
 make clean
 make -j
\end{code}



% ./configure --with-multithreading=omp --with-intel


\begin{remark}
 It is confirmed that the Python module should not load gcc, and that this will
 be removed in future installations.
 In the meantime, the unload is required.
 If you skip it, then the Intel linker will fail.
\end{remark}

\noindent
The Java module is again required if and only if you use the legacy DaStGen
feature to model data structures.
If you decide to use the C++ multithreading, you can leave out the TBB-specific
settings.


For Intel's ITAC, we have to load the itac module and reconfigure:
\begin{code}
module itac/2020.2
setenv CXXFLAGS "-DLogService=ITACLogger -I/ddn/apps/Cluster-Apps/intel/2019.5/tbb/include -I"$ITAC_HOME"/itac_2020/intel64/include"
./configure ... --with-intel 
\end{code}



\section{COSMA/DINE (Durham Intelligent NIC Environment)}


Log into the first DINE node as by documentation (\texttt{ssh -X b101}) and run

\begin{code}
module load gnu_comp/10.2.0
module unload python/2.7.15
module load python/3.6.5
./configure CXXFLAGS="-I/usr/mpi/gcc/openmpi-4.0.3rc4/include" LDFLAGS="-L/usr/mpi/gcc/openmpi-4.0.3rc4/lib64/" --enable-exahype --with-multithreading=omp --with-mpi=/usr/mpi/gcc/openmpi-4.0.3rc4/bin/mpicxx
\end{code}

/usr/mpi/gcc/openmpi-4.0.3rc4/bin/mpirun 


On COSMA 7, I have successfully worked with the following toolchain:


\begin{code}
module load oneAPI/2021.1.8
module load intel_comp/2020-update2
module load itac/2021.1-beta08
module unload python
module load python/intelpython3
export PYTHONPATH=../../../python

./configure CXXFLAGS=-I${VT_ROOT}/include --enable-exahype --with-mpi=mpiicpc --with-multithreading=omp --with-toolchain=intel
\end{code}


\noindent
With TBB, I had to fall back to the manual/minimalist tools:
\begin{code}
module load intel_comp/2020-update2
module load intel_mpi/2020-update2
module unload python
module load python/intelpython3
export PYTHONPATH=../../../python

./configure LDFLAGS="-L${TBBROOT}/lib/intel64/gcc4.8 -ltbb_debug" --enable-exahype --with-mpi=mpiicpc --with-multithreading=tbb
\end{code}



No success so far with the OneAPI environment:
\begin{code}
module load oneAPI/2021.1.8-all
./configure --enable-exahype --with-mpi=mpiicpc --with-multithreading=tbb --with-toolchain=intel \
 CXXFLAGS="-I${VT_ROOT}/include -I/cosma/local/intel/oneAPI_2021.1.8/tbb/2021.1-beta08/include" \
 LDFLAGS="-L${VT_LIB_DIR} ${VT_ADD_LIBS}" 

\end{code}


I have not been able to see any difference with the ucx module. It seems that
we've well hidden most of the exchange.




Sollte nur oneapi sein, nciht das all




% ./configure CXXFLAGS=-I${VT_ROOT}/include LDFLAGS="-L${TBBROOT} -ltbb_debug" --enable-exahype --with-mpi=mpiicpc --with-multithreading=tbb --with-toolchain=intel



\section{Bede}


Access to Bede can be requested through SAFE
(\url{https://safe.epcc.ed.ac.uk/}).
The machine then is reachable through \texttt{ssh} with \texttt{ssh
<username>@bede.dur.ac.uk}


\paragraph{Prepare the Python and bash environment}

You will need to run this option to obtain a proper compile environment:
\begin{code}
echo builder > ~/.application_environment
\end{code}
and log out and log in again.


To install Python with all packages, run
\begin{code}
python3 -m venv $HOME/peano-python-api
source $HOME/peano-python-api/bin/activate
pip install jinja2 sympy
\end{code}


\paragraph{Build with the GNU compilers}

To build \Peano:
\begin{code}
. /opt/software/spack/share/spack/setup-env.sh
module purge
module load spack
spack load gcc@10.2.0
spack load cuda
export CXXFLAGS="-fopenmp"
export LDFLAGS="-lgomp"
./configure --with-multithreading=omp \
   --with-toolchain=nvidia \
   CXXFLAGS="-fopenmp -foffload=nvptx-none -lm -fno-fast-math -fno-associative-math" \
   LDFLAGS="-lnvToolsExt -fopenmp"
make clean
make -j 64
\end{code}

\noindent
Don't forget to add \texttt{--enable-exahype --enable-loadbalancing-toolbox} if
you work with \ExaHyPE.

To run an example, create a SLURM script (e.g - slurm.sh) with something like the following:
\begin{code}
#!/bin/bash
#SBATCH --account=bddur01
#SBATCH --time=1:0:0
#SBATCH --export=ALL
#SBATCH --partition=gpu
#SBATCH --nodes=1
#SBATCH --exclusive
#SBATCH --gres=gpu:1  # 1/4 node cpu & mem allocated per gpu
#SBATCH --gpu-bind=closest

 module purge
 module load cuda/10.2.89
 module load gcc/10.2.0

 export CXXFLAGS="-fopenmp"
 export LDFLAGS="-lgomp"

 pushd Peano/examples/exahype2/euler
 export PYTHONPATH=../../../python
 python3 example-scripts/finitevolumes-bede.py
 ./peano4
\end{code}

and to run it:
\begin{code}
  module load slurm
  sbatch slurm.sh
\end{code}

\section{Local workstations}



If you don't have a proper module file, you might have to configure your environment manually to use TBB:
\begin{code}
 export CXXFLAGS=-I/opt/intel/tbb/include
 export LDFLAGS="-L/opt/intel/tbb/lib/intel64/gcc4.7 -ltbb -pthread"
 ./configure --with-multithreading=tbb --with-mpi=/opt/mpi/mpicxx
\end{code}


If you want to build against VTK, you have to set the pathes accordingly. 
On my machine, it looks similar to
\begin{code}
 export CXXFLAGS=-I/opt/vtk/include/vtk-9.0
 export LDFLAGS=-L/opt/vtk/lib64
 ./configure --with-vtk=-9.0
\end{code}


With the Intel tools, you have to ensure that the environment is set up 
properly:
\begin{code}
source /opt/intel/bin/iccvars.sh intel64
source /opt/intel/itac/2020.0.015/bin/itacvars.sh
source /opt/intel/impi/2019.6.166/intel64/bin/mpivars.sh
\end{code}
\label{label:supercomputer:Intel-scripts}


If I wanna use the Fortran/MHD stuff, I need an additional
\begin{code}
source /opt/intel/bin/ifortvars.sh intel64
\end{code}
to make the compiler \texttt{ifort} available to \Peano.


\section{The Intel tools}
\label{section:supercomputers:IntelTools}

\paragraph{Intel's Threading Building Blocks (TBB)}

The above descriptions all include the TBB tools in the release version.
If you want to debug with TBB or to use the Intel tools, then I recommend to
link against \texttt{tbb\_debug} and to set a few extra command line arguments:

\begin{code}
 export PEANO_CXXFLAGS="-DTBB_USE_ASSERT -DTBB_USE_THREADING_TOOLS "
 export LDFLAGS="see-above   -ltbb_debug"
\end{code}

\noindent
Some systems such as SuperMUC-NG provide a dedicated debug environments,
i.e.~here you can use
\begin{code}
 export CXXFLAGS="$TBB_INC -DTBB_USE_ASSERT -DTBB_USE_THREADING_TOOLS "
 export LDFLAGS="$TBB_SHLIB_DEBUG"
\end{code}


\paragraph{Intel Trace Analyser and MPI Correctness Checks}

If you want to use the Intel tracing or the correctness checks, you have to
translate the code with the linker flags \texttt{-trace} or \texttt{check\_mpi},
respectively.
To set them, please use the \texttt{PEANO\_LDFLAGS}.
If you try to overwrite \texttt{LDFLAGS}, configure sometimes complains.


\begin{code}
 export PEANO_LDFLAGS=-trace
 export PEANO_LDFLAGS=-check_mpi
 ./configure --with-multithreading=tbb --with-mpi=mpiicpc
 make clean
 make -j
\end{code}


\noindent
As Peano consists of multiple libraries which are (partially) linked before you
include them into your executable, you have to rebuild the whole project again.
If you don't do this, you will get error messages that data is corrupted or MPI
datatypes are not defined.



\begin{remark}
 I had crashes in the code as soon as I switched
 \begin{code}
 export VT_CHECK_TRACING=on
 \end{code}
 on. So this seems not to work with Peano.
\end{remark}



\begin{remark}
 I had deadlocks in my code with
 \begin{code}
 export LD_PRELOAD=libVTfs.so
 \end{code}
 as soon as I did remove the \texttt{-check\_mpi} flag from \texttt{mpirun}.
\end{remark}



\begin{remark}
 I have not yet succeeded in running Intel's \texttt{-check\_mpi} with \ExaHyPE.
 The checker claims that the code deadlocked, even though it does not. There
 seems to be a problem with the tool (2019 version) and the nonblocking barrier
 used in \ExaHyPE\ to synchronise all ranks.
\end{remark}


\section{Using MUST}
\label{section:supercomputers:MUST}

Using MUST is straightfoward with Peano:
\begin{code}
  export PATH=/usr/local/must/bin/:$PATH
\end{code}


\noindent
You have to be careful however not to use the \texttt{-check\_mpi} flag with the
Intel tools if you want to run MUST.
With the Intel checker, MUST does not produce any output.
Once the MUST path is set properly, you replace the \texttt{mpirun} with
\texttt{mustrun}.
We've checked all core components of Peano to pass through MUST without any
problems.


% 
% \paragraph{Intel MPI}
% 
% If you you Intel MPI, it seems that you now have to add the \texttt{-parallel}
% flag to your makefile if you use the Python API. Otherwise, your linker will
% fail.
% I haven't yet found out which ``misconfiguration'' in Peano4 causes this
% dependency.
% As the flag is not required on other systems, the Python's API output does not
% by default add this field.
% So either modify the makefile manually or set it as additional \texttt{LDFLAGS}
% entry before you compile.
% The core b.t.w.~seems to be fine without the flag for whatever reason.

 
% nimm das -openmp raus aus dem Script, dann braucht es das -parallel im Linker net
% 
% -mt_mpi
% 
% 
% mpiexec is hte only support thing. no mpirun no srun
% 
% https://doku.lrz.de/display/PUBLIC/Job+Processing+with+SLURM+on+SuperMUC-NG
% 
% 
% 
% 
% https://software.intel.com/en-us/application-snapshot-user-guide-creating-configuration-file-for-intel-trace-collector
% 
% https://doku.lrz.de/display/PUBLIC/Intel+Tracing+Tools%3A+Profiling+and+Correctness+checking+of+MPI+programs#IntelTracingTools:ProfilingandCorrectnesscheckingofMPIprograms-ConfigurationFile
% 
% 
% 
% 
