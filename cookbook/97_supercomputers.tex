\chapter{Selected HPC platforms}


\section{SuperMUC-NG}

%
Some remarks on Peano4 on SuperMUC-NG:


\begin{code}
 ssh lu57zat6@skx.supermuc.lrz.de
 cd $WORK
 export CXX icpc
\end{code}

\noindent
All Intel tools are properly preloaded on SuperMUC-NG.
Hence, we can directly translate the code with MPI. 
However, TBB seems not to be a standard package within the Intel module anymore. 
So if you need TBB, you have to load it manually.

\begin{code}
module load tbb
export CXXFLAGS="$TBB_INC  -DTBB_USE_ASSERT -DTBB_USE_THREADING_TOOLS"
export LDFLAGS="$TBB_SHLIB"
./configure --with-multithreading=tbb --with-mpi=mpiicpc
\end{code}


% So geht es auch auf dem Login-Knoten:
% export I_MPI_HYDRA_BOOTSTRAP=fork

\noindent
If you want to use the legacy DaStGen variant with Java, you will have to load
Java manually.
Python3 in contrast is preloaded by default.



\paragraph{Intel Trace Analyser and Correctness Checks}


We follow the description from \url{https://software.intel.com/en-us/itc-user-and-reference-guide-configuring-intel-trace-collector}:

\begin{itemize}
  \item Run your code with mpirun but pass the arguments \texttt{-check_mpi}, too. This will create a file \texttt{peano4.stf}.
  \item Run \texttt{itcconfig peano4.stf}. For this, you need X-forwarding.
  \item Let the environment variable point to the new config file: \texttt{export VT_CONFIG=peano4.conf}.
  \item Rerun the test with the \texttt{check_mpi} argument.
\end{itemize}




Thread and Signal Safety
The MPI standard requires that MPI_Finalize be called only by the same thread that initialized MPI with either MPI_Init or MPI_Init_thread.


nimm das -openmp raus aus dem Script, dann braucht es das -parallel im Linker net

-mt_mpi


mpiexec is hte only support thing. no mpirun no srun

https://doku.lrz.de/display/PUBLIC/Job+Processing+with+SLURM+on+SuperMUC-NG




https://software.intel.com/en-us/application-snapshot-user-guide-creating-configuration-file-for-intel-trace-collector

https://doku.lrz.de/display/PUBLIC/Intel+Tracing+Tools%3A+Profiling+and+Correctness+checking+of+MPI+programs#IntelTracingTools:ProfilingandCorrectnesscheckingofMPIprograms-ConfigurationFile




export VT_CONFIG=configure-itac.conf



vi configure-itac.conf
a






\section{Hamilton}

Some remarks on Peano4 on Hamilton, Durham's local supercomputer:

\begin{code}
 ssh frmh84@hamilton.dur.ac.uk
 module purge
 # module load intel/xe_2018.2
 module load intel/2019.5
 module load intelmpi/intel/2019.6
 # for the Python API
 module load python/3.6.8 
 module unload gcc/8.2.0
 # for legacy DaStGen
 module load java/1.8.0
 cd $SCRATCH
 cd /ddn/data/frmh84/
 setenv CXX icpc
 ./configure --with-multithreading=what-you-prefer --with-mpi=mpicxx
\end{code}

\begin{remark}
 It is confirmed that the Python module should not load gcc, and that this will
 be removed in future installations.
 In the meantime, the unload is required.
 If you skip it, then the Intel linker will fail.
\end{remark}

\begin{remark}
The Java module is required if and only if you use the legacy DaStGen feature to
model data structures.
\end{remark}


\paragraph{Intel MPI}

If you you Intel MPI, it seems that you now have to add the \texttt{-parallel}
flag to your makefile if you use the Python API. Otherwise, your linker will
fail.
I haven't yet found out which ``misconfiguration'' in Peano4 causes this
dependency.
As the flag is not required on other systems, the Python's API output does not
by default add this field.
So either modify the makefile manually or set it as additional \texttt{LDFLAGS}
entry before you compile.
The core b.t.w.~seems to be fine without the flag for whatever reason.



\paragraph{Intel's Threading Building Blocks (TBB)}

Intel's Threading Building Blocks are available through the environment
variables \texttt{TBB\_INC} and \texttt{TBB\_SHLIB}, as soon as you have 
loaded the Intel compiler.
To pipe them into configure, we have to map them onto the appropriate autotools
parameters:

%  setenv TBB_INC "-I/ddn/apps/Cluster-Apps/intel/xe_2018.2/tbb/include"
%  setenv TBB_SHLIB "-L/ddn/apps/Cluster-Apps/intel/xe_2018.2/tbb/lib/intel64/gcc4.7 -ltbb"
 
\begin{code}
 setenv TBB_INC "-I/ddn/apps/Cluster-Apps/intel/2019.5/tbb/include"
 setenv TBB_SHLIB "-L/ddn/apps/Cluster-Apps/intel/2019.5/tbb/lib/intel64/gcc4.7 -ltbb"
 
 setenv CXXFLAGS "$TBB_INC  -DTBB_USE_ASSERT -DTBB_USE_THREADING_TOOLS"
 setenv LDFLAGS "$TBB_SHLIB -ltbb_debug"
 setenv CXX icpc
 ./configure --with-multithreading=tbb --with-mpi=mpicxx
\end{code}



\section{Local workstations}
If you don't have a proper module file, you might have to configure your environment manually to use TBB:
\begin{code}
export CXXFLAGS=-I/opt/intel/tbb/include
export LDFLAGS="-L/opt/intel/tbb/lib/intel64/gcc4.7 -ltbb -pthread"

./configure --with-multithreading=tbb --with-mpi=/opt/mpi/mpicxx
\end{code}

I found bash sometimes to be picky when setting the TBB libraries. The path and the library literally have to be set separately.






