\section{MPI parallelisation}
  \label{section:parallelisation:mpi}


\chapterDescription
  {
    180 minutes.
  }
  {
    A working simulation code and working MPI environment.
  }

In this section, we discuss how to parallelise a simulation code with message
passing.

\subsection{Preparation}

Peano relies on MPI with its SPMD paradigm, i.e.~all ranks run exactly the same
code.
Internally, it however creates a logical tree topology on all ranks. 
There is one rank that is the {\em global master}.
By convention, this is rank 0, i.e.~the very first rank launched by MPI.
All other ranks are {\em workers}.
Each rank besides the global master has one unique {\em master}.
Each worker can either be active, i.e.~participate in the computation, or it can
be idle.
At the begin of the simulation, all workers are idle and only the global master
is working (as it is working always).

The global master runs the actual Peano simulation code, holds the simulation
state and triggers the grid traversals.
Whenever it tiggers a grid traversal, it tells all non-idle workers which
adapter is currently used.
It determines which algorithmic phase to run. 
Then, it starts to run through the grid. 
If parts of the grid are deployed to other ranks, they are informed to start a
traversal as well. 
The traversal kick-off propagates throught the ranks along the master-worker
topology.
All of this is done automatically.
Most codes require the programmer to think only about the global master.

\paragraph{Re-translating the code.}
We expect that there is a command \texttt{mpicxx} available in your path.
This command often also is called \texttt{mpiCC}. Please ensure it calls the
right compiler backend. 
If you are unsure, check with argument \texttt{-V} and adopt the backend
manually if required. 

Change your compile command to \texttt{mpicxx} and add the two options
\begin{code}
  -DParallel -DMPICH_IGNORE_CXX_SEEK
\end{code}
to your compiler call.
While Peano is written in C++11, it relies only on C bindings of MPI.
Some MPI versions (mpich) have issues with this combination (give you tons of
warnings) unless you pass \texttt{MPICH\_IGNORE\_CXX\_SEEK}. 


\paragraph{Setting up the code.}

The autogenerated \texttt{main} calls Peano's operation \newline
\texttt{peano::initParallelEnvironment} which takes care of a proper MPI
initialisation. 
As soon as the initialisation has terminated without an error code, you may use
the predicate

\begin{code}
if (tarch::parallel::Node::getInstance().isGlobalMaster()) {
  ...
}
\end{code}

\noindent
to find out whether a particular piece of code runs on the global master. 
One of the first steps in many codes might be to perform some tests only on this
global master.
The counterpart \texttt{peano::shutdownParallelEnvironment()} typically is
invoked directly within \texttt{main} as well and is also called by the
autogenerated templates already.

As we follow SPMD, \texttt{main} has to create an instance of the runner and
invoke its \texttt{run} operation. 
The \texttt{run} then distinguishes between the global master and all the other
workers that blindfolded follow their masters. 
\begin{code}
  int result = 0;
  if (tarch::parallel::Node::getInstance().isGlobalMaster()) {
    result = runAsMaster( *repository );
  }
  #ifdef Parallel
  else {
    result = runAsWorker( *repository );
  }
\end{code}

\noindent
This code is pregenerated and it is most of the time sufficient to focus on the
operation \texttt{runAsMaster}, i.e.~to focus on the serial code version. There
are however a few steps that have to be done on each individual worker
separately. 
You may implement this within the \texttt{main}.
However, we typically realise it within \texttt{run} just before or after the
operation splits up into the function for the global master and all other ranks.


One for the first steps {\bf ran only on the global master} is to configure a
proper load balancing strategy. 
Peano realises a hybrid centralised-decentralised load balancing by default,
where load balancing decisions are, whenever possible, made decentral. 
Once load balancing decisions are made (along the lines `I would like more
ranks to help me with my work'), a central point is contacted that decides which
ranks are involved in the rebalancing.
This central point is called {\em node pool}.
Peano realises the whole node pool, but allows the user to plug into the
decisions which ranks are assigned to help which other ranks, e.g.
We have to configure the node pool on the global master only
\begin{code}
if (tarch::parallel::Node::getInstance().isGlobalMaster()) {
  tarch::parallel::NodePool::getInstance().setStrategy(
    new tarch::parallel::FCFSNodePoolStrategy()
  );
}
\end{code}

\noindent
where we select here one of the default strategies. 
It answers to any incoming load balancing request FCFS (while other strategies
might decide to bundle requests to get an overview who asks for resources first
before any rank assignment is performed) and hands out MPI ranks as long as
there are still idle workers available.

Once the node pool is initialised, we have to restart it. 
This has to be done {\bf on all ranks}. 
On the global master, it really restarts the node pool.
On all other ranks, it establishes the connection to the central node pool
and informs the latter how many ranks are available.

\begin{code}
  tarch::parallel::NodePool::getInstance().restart();
\end{code}

\noindent
In a next step, we configure the actual load balancing. 
Again, we rely on a default balancing that simply yields a static partitioning. 
This partitioning is established by a greedy grid decomposition while the grid
is built up.

\begin{code}
  peano::parallel::loadbalancing::Oracle::getInstance().setOracle(
    new mpibalancing::StaticBalancing(true)
  );
\end{code}

\noindent
This trivial load balancing runs embarassingly parallel on all ranks. 
The ranks do not communicate to come up with load balancing decisions.
In such a case, you could use different oracles on different ranks. 
In general, this is not a good idea and all ranks should run the same oracle.
 
 
MPI code tends to be very sensitive to proper buffer sizes.
While the default buffer sizes in Peano should be properly chosen, you can
always change them if you like.
However, you have to ensure that all ranks work with the same buffer sizes
(otherwise the exchange of buffers will fail):

\begin{code}
peano::parallel::SendReceiveBufferPool::getInstance().setBufferSize( bufferSize );
peano::parallel::JoinDataBufferPool::getInstance().setBufferSize( bufferSize );
\end{code}


\noindent
Peano's MPI communication routines all have some built-in deadlock detection. 
To enable it, you have to quantify what MPI waiting time is to be considered to
be a deadlock.
The deadlock identification is split into two phases.
A certain timeout interval first has to pass.
After it, a warning is launchend. 
After a second timeout passes, the code is terminated.
It is reasonable to set these timeouts rather high if you run your code with
assertions or debug information.
To do all the checks and create debug data is time-consuming and thus might lead
into a deadlock identification though all nodes are busy creating the additional
information.
The other way round, many applications do not allow to run production runs with
assertions and debugs.
As deadlocks for small problems occur sooner than for large problems where
already MPI ill-balancing might yield long waiting times, they might realise the
waiting strategy exactly the other way round.

\begin{code}
  #if defined(Debug) || defined(Asserts)
  tarch::parallel::Node::getInstance().setDeadlockTimeOut(120*4);
  tarch::parallel::Node::getInstance().setTimeOutWarning(60*4);
  #else
  tarch::parallel::Node::getInstance().setDeadlockTimeOut(120);
  tarch::parallel::Node::getInstance().setTimeOutWarning(60);
  #endif
\end{code}

\noindent
It is finally good practice to make the node pool terminate just before
\texttt{run} has terminated.
You might also want to release all MPI datatypes Peano has created, as some
tools do complain if you don't so:

\begin{code}
tarch::parallel::NodePool::getInstance().terminate();
particles::pidt::repositories::RepositoryFactory::getInstance().shutdownAllParallelDatatypes();
\end{code}



\paragraph{Inside \texttt{runAsMaster}.}
\begin{code}
  tarch::parallel::NodePool::getInstance().waitForAllNodesToBecomeIdle();

   while (!repository.getState().isGridBalanced()) {

 
    logInfo(
    "runAsMaster()",
    "number of working ranks=" << tarch::parallel::NodePool::getInstance().getNumberOfWorkingNodes()
  );
  logInfo(
    "runAsMaster()",
    "number of idle ranks=" << tarch::parallel::NodePool::getInstance().getNumberOfIdleNodes()
  );
  
  
  #include "peano/parallel/loadbalancing/Oracle.h"
#include "peano/parallel/loadbalancing/OracleForOnePhaseWithGreedyPartitioning.h"
\end{code}
  
 

\subsection{Exchanging the global state}

Was muss man mit parallelise markerien

\subsection{Exchanging boundary data}

Was muss man mit parallelise markerien

\subsection{Exchanging boundary data on the heap}

\subsection{Running global steps on all ranks}

\subsection{Specifying the communication pattern}

\subsection{Doing something special on a worker}
